{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"install/","title":"Install","text":""},{"location":"install/#prerequisites","title":"Prerequisites","text":"<p>Provide access to Kubernetes cluster. You can also run it locally within a Minikube1 or Kind:</p> minikubeminikube (MacOS)kind <pre><code>minikube start --memory=8196 --cpus 8\n</code></pre> <pre><code>minikube start --driver=hyperkit --memory=8196 --cpus 8\n</code></pre> <pre><code>kind create cluster --name sr-cluster\n</code></pre>"},{"location":"install/#streaming-runtime-operator","title":"Streaming Runtime Operator","text":"<p>This oneliner installs the <code>Streaming Runtime Operator</code> in your Kubernetes environment:</p> <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-operator/install.yaml' -n streaming-runtime\n</code></pre> <p>It installs the <code>streaming-runtime</code> operator along with the custom resource definitions (CRDs) (such as <code>ClusterStream</code>, <code>Stream</code> and <code>Processor</code>) and required roles and binding configurations.</p>"},{"location":"install/#service-binding-operator-optional","title":"Service Binding Operator (optional)","text":"<p>If the Service Binding specification is used to manage the sensitive information in the streaming pipelines,  you need to pre-install a compliant operator such as the VMWare-Tanzu Service Binding Operator:</p> <pre><code>kubectl apply -f https://github.com/vmware-tanzu/servicebinding/releases/download/v0.7.1/service-bindings-0.7.1.yaml\n</code></pre>"},{"location":"install/#rabbitmq-cluster-and-message-topology-operators-optional","title":"RabbitMQ Cluster and Message Topology Operators (optional)","text":"<p>If you decided to use the RabbitMQ auto-provisioning, based on RabbitMQ Cluster &amp; Message Topology Operators the following managers and operators are required:</p> <pre><code>kubectl apply -f \"https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\"\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml\nkubectl apply -f https://github.com/rabbitmq/messaging-topology-operator/releases/latest/download/messaging-topology-operator-with-certmanager.yaml\n</code></pre>"},{"location":"install/#next-steps","title":"Next Steps","text":"<p>Follow the Overview for general understanding how the Streaming Runtime works or the Samples for various executable examples.</p> <ol> <li> <p>Make sure to provision enough memory (8G+) and CPU (8+) resources.\u00a0\u21a9</p> </li> </ol>"},{"location":"sr-technical-stack/","title":"SR Stack","text":""},{"location":"sr-technical-stack/#streaming-lite","title":"Streaming-Lite","text":"<p>For applications which rely on time-ordered delivery of events for a single key, but which do not perform any cross-key operations like joins or repartitioning.  These applications include windowing in some cases but not others.  Typically, streaming-lite applications run as pipelined workloads in an existing scheduler framework such as Kubernetes.  Examples include ETL applications and time-series analysis.</p> <p>The Streaming Runtime SRP and SCS processor types are provided to support the Streaming Light use case. The built-in, general purpose SRP processor is capable of data partitioning, scaling, time-window aggregation and polyglot UDF functions. The SCS processor provide drop-in support for the large Spring Cloud Stream / Spring Cloud Function ecosystems.</p>"},{"location":"sr-technical-stack/#streaming-expert","title":"Streaming-Expert","text":"<p>For applications which perform cross-stream analysis using joins and/or repartitioning, or applications which may perform dynamic stream operations (for example, windowing based on data conditions rather than time or rows). These applications typically using a pool of worker nodes, where the code and data is distributed to nodes using a framework-specific technique. Examples include Apache Spark, Apache Beam, and Apache Flink.</p> <p>Currently the Streaming Runtime offers a FSQL processor type that can run fully-fledged Apache Flink Streaming SQL queries. (in embedded mode only).</p>"},{"location":"sr-technical-stack/#implementation-stack","title":"Implementation Stack","text":"<p>The Streaming Runtime implementation stack looks like this:</p> <p></p> <p>TODO: Provide CRC documentation for the implementation stack.</p>"},{"location":"streaming-runtime-build/","title":"Build &amp; Run","text":""},{"location":"streaming-runtime-build/#streaming-runtime-operator","title":"Streaming Runtime Operator","text":"<p>build instructions to build the operator, create a container image and upload it to container registry.</p>"},{"location":"streaming-runtime-build/#crds","title":"CRDs","text":"<p>Every time the CRDs under the <code>./crds</code> folder are modified make sure to runt the regnerate the models and installation.</p> <ul> <li> <p>Generate CRDs Java api and models <pre><code>./scripts/generate-streaming-runtime-crd.sh\n</code></pre> Generated code is under the <code>./streaming-runtime/src/generated/java/com/vmware/tanzu/streaming</code> folder</p> </li> <li> <p>Build operator installation yaml <pre><code>./scripts/build-streaming-runtime-operator-installer.sh\n</code></pre> producing the <code>install.yaml</code>.</p> </li> </ul> <p>The <code>./scripts/all.sh</code> combines above two steps.</p>"},{"location":"streaming-runtime-build/#build-the-operator-code-and-image","title":"Build the operator code and image","text":"<p><pre><code>./mvnw clean install -Dnative -DskipTests spring-boot:build-image\ndocker push ghcr.io/vmware-tanzu/streaming-runtimes/streaming-runtime:0.0.4-SNAPSHOT\n</code></pre> (For no-native build remove the <code>-Dnative</code>).</p>"},{"location":"streaming-runtime-build/#user-defined-functions","title":"User Defined Functions","text":"<p>Follow the User Defined Function documentation to learn how to implement and build UDFs, and how to use them from within a Processor resource.</p>"},{"location":"streaming-runtime-overview/","title":"Overview","text":"<p>The Streaming Runtime (SR) implements, architecturally, the streaming data processing as a collection of independent event-driven streaming applications, called Processors, connected over a messaging middleware of choice (for example RabbitMQ or Apache Kafka).  The connection between two or more <code>Processors</code> is called Stream: </p> <p></p> <p>The <code>Processor</code> and the <code>Stream</code> 1 are implemented as Kubernetes API extensions, defined as Custom Resources and implementing Reconciliation Controllers for them. Consult the SR technical stack for further implementation details.</p> <p>The <code>Streams</code> CR instance specifies storage-at-rest of time-ordered attribute-partitioned, structured data, such as a Kafka topic, or RabbitMQ exchange/queue. This specification is used by the SR controllers to configure and wire the underlining connections.</p> <p>The <code>Processor</code> CR instance defines the manifest of the event-driven streaming application to be deployed by the SR processor controllers. Once deployed the application continuously receives input streaming data, transforms it and sends the results downstream over the outputs. </p> <p>The <code>Processor</code> can have zero or more input and output <code>Streams</code>. The collection of <code>Processors</code> and <code>Streams</code> come together at runtime to constitute streaming <code>Data Pipelines</code> (sometimes referred as <code>Multistage topologies</code>):</p> Simplified diagramFull (with ClusterStream) diagram <p></p> <p></p> <p>The pipelines can be linear or nonlinear, depending on the data flows between the applications.</p> <p>After installing the SR operator, one can use the <code>kind:Stream</code> and <code>kind:Processor</code> resources to define a new streaming application2 manifest:</p> Development stageProduction stage <p>For convenience during the development stage, the SR operator auto-provisions the <code>ClusterStreams</code> for all <code>Streams</code> that don't have explicitly declared them.</p> simple-streaming-app.yaml<pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-in-stream\nspec:\nname: data-in\nprotocol: \"kafka\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: multibinder-processor\nspec:\ntype: SRP # use the built-in processor implementation\ninputs:\n- name: data-in-stream\noutputs:\n- name: data-out-stream\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-out-stream\nspec:\nname: data-out\nprotocol: \"rabbitmq\"\n</code></pre> <p>In production environment the Streaming Runtime will not be allowed to auto-provision ClusterStreams dynamically.  Instead the Administrator will provision the required messaging middleware and declare ClusterStream to provide managed and controlled access for it.</p> <p>The <code>ClusterStreams</code> and the <code>Streams</code> follow the PersistentVolume model: namespaced <code>Stream</code> declared by a developer (ala <code>PVC</code>) is backed by a <code>ClusterStream</code> resource (ala <code>PV</code>) which is controlled and provisioned by the administrator.</p> simple-streaming-app.yaml<pre><code>#################################################\n#  ADMIN responsibility\n#################################################\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: ClusterStream\nmetadata:\nname: kafka-cluster-stream\nspec:\nname: data-in\nstreamModes: [\"read\", \"write\"]\nstorage:\nserver:\nurl: \"kafka.default.svc.cluster.local:9092\"\nprotocol: \"kafka\"\nreclaimPolicy: \"Retain\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: ClusterStream\nmetadata:\nname: rabbitmq-cluster-stream\nspec:\nname: data-out\nstreamModes: [\"read\", \"write\"]\nstorage:\nserver:\nurl: \"rabbitmq.default.svc.cluster.local:5672\"\nprotocol: \"rabbitmq\"\nreclaimPolicy: \"Retain\"\n---\n#################################################\n#  DEVELOPER responsibility\n#################################################\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-in-stream\nspec:\nname: data-in\nprotocol: \"kafka\"\nstorage:\n# Claims the pre-provisioned Kafka ClusterStream.\nclusterStream: kafka-cluster-stream ---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: multibinder-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in-stream\noutputs:\n- name: data-out-stream\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-out-stream\nspec:\nname: data-out\nprotocol: \"rabbitmq\"\nstorage:\n# Claims the pre-provisioned rabbitmq ClusterStream.\nclusterStream: rabbitmq-cluster-stream </code></pre> <p>and submit it to a Kubernetes cluster:</p> <pre><code>kubectl apply -f ./simple-streaming-app.yaml -n streaming-runtime\n</code></pre> <p>On submission, the SR controllers react by provisioning and configuring the specified resources. For example the <code>SRP</code> processor type, defined in the <code>Processor</code> CR, instructs the SR to provision the built-in, general purpose, SRP processor implementation.</p> <p>Likewise if the messaging middleware claimed by the <code>Stream</code> CRs (in this example: Apache Kafka and RabbitMQ) is not available, the controllers for the ClusterStreams will detect and auto-provision it.</p> <p>Developers have different options to build their own data transformation logic. Depending on the extension approach, processor are grouped in the following types: </p> <ul> <li>SRP Processor - is a general purpose, processor, that allows developers to plug in their own, polyglot User Defined Functions (UDF). The SRP processors provide support for Tumbling Time-Window aggregations and streaming Data Partitioning.</li> <li>SCS Processor - can run any Spring Cloud Stream application natively in Kubernetes. One can choose from the  extensive set (60+) of pre-built streaming applications or build a custom one. The SCS Processor supports stateful and stateless workloads and stream data partitioning3.</li> <li>FSQL Processor - is backed by Apache Flink and supports streaming SQL.  This allows the developers to implement complex data transformation, such as stream join or windowed aggregation, by defining streaming SQL queries.</li> </ul> <p>Developers can choose and mix the processor types interchangeably when implementing the streaming applications for their data pipelines.</p>"},{"location":"streaming-runtime-overview/#next-steps","title":"Next Steps","text":"<p>After installing the streaming runtime, follow the Examples for various executable examples.</p> <ol> <li> <p>The Streaming Runtime Operator provides also a ClusterStreams resources, that are responsible to provision the messaging middleware used by the Streams.  The <code>ClusterStreams</code> and the <code>Streams</code> follow the PersistentVolume model: namespaced <code>Stream</code> declared by a developer (ala <code>PVC</code>) is backed by a <code>ClusterStream</code> resource (ala <code>PV</code>) which is controlled and provisioned by the administrator. For convenience during the development stage, the SR operator auto-provisions the <code>ClusterStreams</code> for all <code>Streams</code> that don't have explicitly declared them.\u00a0\u21a9</p> </li> <li> <p>The sample app itself acts as a message bridge. It receives input messages from Apache Kafka, <code>data-in</code> topic and re-transmits them, unchanged, to the output RabbitMQ <code>data-out</code> exchange.\u00a0\u21a9</p> </li> <li> <p>The SCS Processor also provides limited support for polyglot applications.\u00a0\u21a9</p> </li> </ol>"},{"location":"architecture/cluster-streams/overview/","title":"Cluster Streams","text":"<p>The Streaming Runtime Operator provides <code>ClusterStreams</code> allowing operators install dynamic Cluster Stream provisioners for developers to consume and create streams e.g. Kafka topics, or they may choose to limit creation of topics to administrators.</p> <p></p> <p><code>ClusterStreams</code> contains the information where the stream cluster is and its bindings.</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: ClusterStream\nmetadata: {}\nspec:\n# Topic/Exchange name\nname: \"topicOrExchangeNme\"\n# Key attributes for the topic\nkeys: [&lt;string&gt;]\n# Streaming modes that will be allowed at the creation of Streams e.g. read, write\nstreamModes: [&lt;string&gt;]\nstorage:\n# Information about the Message Broker to assess and manage.\nserver:\n# Reference to an existing Service Binding Service (e.g. secrets).\nbinding: &lt;string&gt;\n# Message Broker connection URL\nurl: &lt;string&gt;\n# Message Broker type\nprotocol: &lt;string&gt;\nreclaimPolicy: &lt;string&gt;\nattributes:\n# (optional) message-broker auto-provisioning adapter name\nprotocolAdapterName: &lt;auto-provisioning adapter name&gt;\n</code></pre> <p>For a detailed description of attributes of the resource please read cluster-stream-crd.yaml</p>"},{"location":"architecture/cluster-streams/overview/#stream-relationship","title":"Stream relationship","text":"<p>The <code>ClusterStreams</code> and the Streams follow the PersistentVolume model: namespaced <code>Stream</code> declared by a developer (ala <code>PVC</code>) is backed by a <code>ClusterStream</code> resource (ala <code>PV</code>) which is controlled and provisioned by the administrator. For convenience during the development stage, the SR operator auto-provisions the <code>ClusterStreams</code> for all <code>Streams</code> that don't have explicitly declared them.</p>"},{"location":"architecture/cluster-streams/overview/#service-binding","title":"Service Binding","text":"<p>The Service Binding Specification provides a Kubernetes-wide specification for communicating service secrets to workloads in an automated way. The Stream <code>spec.binding</code> allow to refer existing service binding service (aka secrets).</p>"},{"location":"architecture/cluster-streams/overview/#message-brokers-auto-provisioning","title":"Message Brokers Auto-provisioning","text":"<p>The ClusterStream's controller uses the <code>spec.storage.server</code> information to connect to the target messaging broker (aka Apache Kafka, RabbitMQ and so.) and apply the configured policies. If the target messaging broker is missing, by default the ClusterStream controller will try to auto-provision one. This behavior is handy in the development stage where the developer is not bothered to provision the messaging infrastructure  and instead lets the SR auto-provision and tear down the messaging infrastructure on demand. In production environments this behavior is likely to be disabled. </p> <p>Currently The ClusterStream Controllers can auto-provision:</p> <ul> <li>Apache Kafka with Schema Registry and Kafka UI console.</li> <li>RabbitMQ. You can use the simple rabbitmq deployment (default) or leverage the RabbitMQ Operator by setting <code>protocolAdapterName</code> attribute to <code>rabbitmq-operator</code>. RabbitMQ OperatorSample.</li> </ul>"},{"location":"architecture/cluster-streams/overview/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Ability to reference an already-existing stream to support interoperability with other systems</li> <li>Knowledge and documentation of the partitioning and schema of the stream data</li> <li>Ability to provision new streams and set the partition key</li> <li>Stream status should provide a Duck-type contract which provides all the necessary information to consume the stream once provisioned.</li> </ul>"},{"location":"architecture/data-partitioning/data-partitioning/","title":"Data Partitioning","text":"<p>Info</p> <p>Applicable for the SRP and SCS processor types.  The FSQL processors use the Apache Flink built-in data partitioning capabilities.</p> <p>Partitioning is an essential concept in stateful data processing.  It permits consistent data scaling by ensuring that all related data is processed together. Logically it implements the <code>group by key</code> processing.  For example, in a time-windowed average calculation example, all measurements from any given sensor are processed by the same Processor instance. </p> <p>Technically, the partitioning allows content-based routing of payloads to the downstream processor instances.  This is especially useful when you want to have your downstream processor instances to process data from specific partitions from the upstream processor.  For instance, if a processor application in the data pipeline is performing operations based on a unique identifier from the payload (such as team name), the stream can be partitioned based on that unique identity.</p> <p>Some messaging middleware (such as Apache Kafka) provide additional guarantees that the data in the partitions remains ordered!</p> <p>The Streaming Runtime provides a simple construct to enable and configure stateful data partitioning. On the Steam resource that represents the partitioned connection, use the <code>spec.key</code> or <code>spec.keyExpression</code> to define the what header or payload field to use as a discriminator to partition the data in the steam.  Additionally use the <code>spec.partitionCount</code> property to configure the number of partitions you would like the incoming data to be partitioned into.  Those properties are used to instruct the upstream processor(s) to provision the data partitioning configuration while the downstream processors are configured for partitioned inputs (e.g. enforce instance ordering and stateful connections). </p> <p>If the downstream processor is scaled out (e.g. <code>replications: N</code>), then the streaming runtime will ensure StatefulSet replication instead of <code>Deployment/ReplicationSet</code>. Additionally, for the processors consuming partitioned Stream, the SR configures Pod's Ordinal Index to be used as partition instance-index.  Later ensures that event after Pod failure/restart the same partitions will be (re)assigned to it.</p> <p>For example let\u2019s consider an online-game statistics use case where we want to partition the user data by the team field. Using the SR constructs we can define pipeline like this:</p> <p></p> <p>The input (data-in) contains user gaming statistics such as score, and team the user is a member of.  This data is fed into the user-partition Processor and in turn sent downstream via the partition-by-team Steam configured to partition the data by team name into 3 partition groups.  The downstream (user-score-processor) consumes the partitioned input and because it deploys 3 stateful instances we could expect that each partition will be assigned to a single processor instance.</p> <p></p> <p>Learn how to build partitioned time-window aggregations with the SRP processor.</p> <p>Also visit the following example to learn how to define and configure partitioned streaming pipelines:</p> <ul> <li>Partition by Field with Stateful Replication</li> <li>Partition by Field using Header Keys</li> <li>Partition by Field with replicated Time-Window aggregation</li> <li>Online Gaming Statistics</li> </ul>"},{"location":"architecture/processors/overview/","title":"Processors","text":"<p>The <code>Processor</code> represents an independent event-driven streaming application that can consume one or more input Streams, transform the received data and send the results downstream over one or more output Streams. </p> <p>For a detailed description of attributes of the resource please read processor-crd.yaml</p> <p></p> <p>The Streaming Runtime provides a built-in, general purpose Processor of type SRP and two additional processor types to provide integration with 3rd party streaming technologies, such as Apache Flink (type: FSQL) and Spring Cloud Stream/Spring Cloud Function (type: SCS).  Processors from all types can be combined and used interchangeably.</p> <p>The Streaming Runtime allows implementing additional Processor types that can provide integration with other streaming systems such as Apache Spark, KSQL and alike.</p>"},{"location":"architecture/processors/overview/#processor-types","title":"Processor types","text":"<ul> <li>SRP: Streaming Runtime Processor. Processor built-in the Streaming Runtime, that allow various streaming transformation,  such as message brokers bridging, custom user-defined functions in the language of choice and simple tumbling time-window aggregation.</li> <li>FSQL: Backed by Apache Flink SQL Streaming. Allow inline streaming SQL queries definition.</li> <li>SCS: Runs Spring Cloud Stream applications as processors in the pipeline.</li> </ul>"},{"location":"architecture/processors/fsql/overview/","title":"Apache Flink SQL Processor (FSQL)","text":"<p>Backed by Apache Flink Streaming SQL, it allows inline Query definitions to be defined in the CR resource.  The set of input stream data which should trigger a transformation is represented by a (streaming) SQL query across the various inputs which yields event tuples which are emitted to the output streams.</p> <p>Like the other processor types, the <code>FSQL</code> uses the Processor CRD custom resource to configure the processor within the SR Control Plane. </p> <p>The <code>spec.inlineQuery</code> property should contain one or more, Apache Flink compliant, streaming SQL queries. The FSQL processor does NOT use the <code>spec.inputs</code> and <code>spec.outputs</code> to configure the inbound or outbound Streams.  Instead those are configured within the SQL queries using the following placeholder convention: <code>[[STREAM:&lt;stream-name&gt;]]</code> , where the <code>&lt;stream-name&gt;</code> should refer to an existing <code>Stream</code> CR name.</p> <p>The sql-aggregator event-driven application implements the Data Plane FSQL capabilities. It is implemented as SpringBoot app that runs embedded Apache Flink.</p>"},{"location":"architecture/processors/fsql/overview/#fsql-attributes","title":"FSQL Attributes","text":"<p>Few specific attributes are used to configure the FSQL specific debug capabilities.</p> FSQL Attribute Description <code>debugQuery</code> Optionally you can configure a side query that is continuously against input and output processor's streams and show the result in container's console log. Handy to debug processor output results correctness. <code>debugExplain</code> print the SQL queries logical and physical plans for selected (by index) inline queries. Indices correspond to the inlineQuery order"},{"location":"architecture/processors/fsql/overview/#usage","title":"Usage","text":"<pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata: {}\nspec:\n# Type of the processor. In this case FSQL\ntype: FSQL\n# List of streaming SQL queries that will be executed by the processor. \n# Queries are executed in the order of their definition.\n# Uses in-query stream name placeholders: [[STREAM:&lt;stream-cr-name&gt;]]\ninlineQuery:\n- &lt;string sql&gt;\nattributes:\n# Optionally you can configure a side query that is continuously \n# against input and output processor's streams and show the result \n# in container's console log.\n# Handy to debug processor output results correctness.\ndebugQuery: &lt;string sql&gt;\n# (optional) Additionally can print the SQL queries logical \n# and physical plans for selected (by index) inline queries. \n# Indices correspond to the inlineQuery order.\ndebugExplain: &lt;list of integers&gt;\n</code></pre> <p>Example FSQL definition:</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: topk-songs-join\nspec:\ntype: FSQL\ninlineQuery:\n- \"INSERT INTO [[STREAM:songplays-stream]] SELECT Plays.song_id, Songs.album, Songs.artist, Songs.name, Songs.genre, Plays.duration, Plays.event_time   FROM(SELECT * FROM [[STREAM:playevents-stream]] WHERE duration &gt;= 30000) AS Plays INNER JOIN [[STREAM:songs-stream]] as Songs ON Plays.song_id = Songs.song_id\"\nattributes:\ndebugQuery: \"SELECT * FROM SongPlays\" </code></pre> <p>The <code>inlineQuery</code> continuously joins the input <code>songs-stream</code> and <code>playevents-stream</code> streams on the <code>song_id</code> field and writes the projected, results in the <code>songplays-stream</code> output stream. A sub-query is used to filter in only the playevents-stream events that have <code>duration &gt;= 30000</code>.</p> <p>The <code>debugQuery</code> helps to print in the processor's container log the last join results. Note that the <code>SongPlays</code> is the name of the Stream <code>spec.name</code> not the <code>metadata.name</code> used in the placeholders.</p>"},{"location":"architecture/processors/fsql/overview/#examples","title":"Examples","text":"<ul> <li> Anomaly Detection (FSQL, SRP)- detect, in real time, suspicious credit card transactions, and extract them for further processing.</li> <li> Clickstream Analysis (FSQL, SRP) -   for an input clickstream stream, we want to know who are the high status customers, currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day.</li> <li> IoT Monitoring analysis (FSQL, SRP) - real-time analysis of IoT monitoring log.</li> <li> Streaming Music Service (FSQL, SRP) - music ranking application that continuously computes the latest Top-K music charts based on song play events collected in real-time.</li> </ul>"},{"location":"architecture/processors/scs/overview/","title":"Spring Cloud Stream Processor (SCS)","text":"<p>Runs Spring Cloud Stream applications as processors in the pipeline. One can choose for the extensive set (60+) of pre-built streaming applications or build a custom one. It is possible to build and deploy polyglot applications as long as they interact with the input/output streams manually.</p>"},{"location":"architecture/processors/scs/overview/#usage","title":"Usage","text":"<pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\n# Name of the source of the Spring Cloud Stream.\n# List: https://docs.spring.io/stream-applications/docs/2021.1.2/reference/html/#sources\nname: &lt;string&gt;\nspec:\n# Type of the processor. In this case SCS (Spring Cloud Stream)\ntype: SCS\n# Input Stream name for the processor to get data from\ninputs:\n- name: &lt;string&gt;\n# Output Stream name for the processor to send data\noutputs:\n- name: &lt;string&gt;\ntemplate:\nspec:\n# Container for the Spring Cloud Stream image.\ncontainers:\n- name: &lt;string&gt;\nimage: springcloudstream/&lt;processor&gt;:&lt;tag&gt;\n# List of environment variables that are required for the processor.\nenv:\n</code></pre>"},{"location":"architecture/processors/scs/overview/#examples","title":"Examples","text":"<ul> <li>Spring Cloud Stream pipeline (SCS) - show how to build streaming pipelines using Spring Cloud Stream application as processors.</li> <li>streaming-pipeline-ticktock-partitioned-better.yaml example shows how to data-partition the TickTock application leveraging the SCS Data-Partitioning capabilities.</li> </ul>"},{"location":"architecture/processors/srp/overview/","title":"Streaming Runtime Processor (SRP)","text":"<p>The SRP processor provides generic streaming data processing capabilities such as message brokerage, inline streaming transformations, polyglot user-defined functions, simple tumbling time-window aggregation and data-partitioning capabilities to name a few.</p> <p>It uses Processor CRD based custom resources to configure the processor within the SR Control Plane. </p> <p>The srp-processor event-driven application implements the Data Plane SRP capabilities.</p>"},{"location":"architecture/processors/srp/overview/#srp-attributes","title":"SRP Attributes","text":"<p>Few specific attributes that start with the <code>srp.</code> prefix are used to configure the SRP specific capabilities.</p> SRP Attribute Description <code>srp.envs:</code> Adds random environment variable to the SRP container configuration. Example: <code>\"TEST_BAR=FOO;TEST_FOO=BAR\"</code> <code>srp.output.headers</code> Adds headers to the output messages. Uses expression like: <code>\"user=header.fullName;team=payload.teamName\"</code> <code>srp.window</code> Defines the Time-Window aggregation interval. Examples: <code>5s</code>, <code>2m</code>, <code>1h</code> <code>srp.window.idle.timeout</code> Defines an interval of inactivity to release the idle windows. Should be larger than the window interval! Example: <code>2m</code> <code>srp.input.timestampExpression</code> JsonPath expression Example: <code>header.eventtime</code>, or <code>payload.score_time</code>. Note: It is advised to use the inbound Stream's timeAttributes instead. <code>srp.maxOutOfOrderness</code> Supported out of orderness time. Example: <code>2s</code>. Note: It is advised to use the inbound Stream's timeAttributes instead. <code>srp.allowedLateness</code> Max time to allow late events. (Example <code>1h</code>). <code>srp.lateEventMode</code> Defines the policy to deal with late event records. Supports: <code>DROP</code>, <code>UPSERT</code>, <code>SIDE_CHANNEL</code> modes (defaults to <code>DROP</code>). <code>srp.input.schemaRegistryUri</code> configure the Schema Registry uri. Required for <code>Avro</code> content types. <code>srp.skipUdf</code> Forcefully disables the the UDF call. Defaults to false. Note that if you don't provide side-car container this would effectively skip the UDF. <code>forceStatefulSet</code> If replication is larger than 1 and the forceStatefulSet is set to true then the SR will deploy the processor as <code>StatefulSet</code> event if there is no partitioned input. Defaults to `false'. (applicable for SRP and CSC processors) <code>srp.spel.expression</code> set an inline, <code>SpEL</code> expressions as a data transformation function. <p>Following snippets shows a sample SRP configuration. It uses the <code>srp.</code> attributes to configure tumbling time-window aggregation interval of 5 seconds, with idle timeout of 60 secs. The late events will be send to dedicated side channel. Also an aggregation UDF is registered to aggregate the temporal aggregates.</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: user-scores-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in-stream\noutputs:\n- name: user-scores-stream\nattributes:\nsrp.window: 5s # Tumbling Time Window of 5 seconds.\nsrp.window.idle.timeout: 60s # Allow partial release of idle time-windows.\nsrp.lateEventMode: SIDE_CHANNEL # Send late events a side-channel stream. By default late events are discarded.\ntemplate:\nspec:\ncontainers:\n- name: scores-by-user-javascript\n# The UDF implementation\nimage: ghcr.io/vmware-tanzu/streaming-runtimes/user-score-js:latest\n</code></pre>"},{"location":"architecture/processors/srp/overview/#message-transformation-options","title":"Message Transformation Options","text":"<p>The SRP offers 4 ways to implement event transformation logic:</p>"},{"location":"architecture/processors/srp/overview/#message-broker-bridge","title":"Message Broker Bridge","text":"<p>If no inline transformation or UDF functions are configured, the SRP processor simply retransmits the inbound events, unchanged, to the outbound Streams. This could be useful to implement Message Broker Bridges.</p>"},{"location":"architecture/processors/srp/overview/#inline-transformations","title":"Inline Transformations","text":"<p>The <code>srp.spel.expression</code> attribute allows setting an inline, SpEL expressions as a transformation function. SpEL is a powerful expression language that supports querying and manipulating an messages at runtime. As Message format has two parts (<code>headers</code> and <code>payload</code>) that allow SpEL expressions such as <code>payload</code>, <code>payload.thing</code>, <code>headers['my.header']</code>, and so on. The inline-transformation example shows how to apply <code>JsonPath</code> expressions to transform the inbound JSON payload.</p>"},{"location":"architecture/processors/srp/overview/#header-enrichment","title":"Header Enrichment","text":"<p>The <code>srp.output.headers</code> attribute allows enriching the outbound message headers with values computed form the inbound message header or payload. For example the  <code>srp.output.headers: \"user=payload.fullName\"</code> expression would add a new outbound header named <code>user</code> with value computed from the inbound payload field <code>fullName</code>. Expression support any structured payload data formats such as Json, AVRO and so on.</p>"},{"location":"architecture/processors/srp/overview/#user-defined-functions-polyglot","title":"User Defined Functions (Polyglot)","text":"<p>Polyglot User Defined Function (UDF). The Streaming Runtime allows implementing the message transformation logic in the language of your choice, packaged in a standalone image container and deployed as a sidecar in the same Pod along with the SRP Processor.</p> <p></p> <p>The communication between the SRP Processor and the UDF running in the sidecar is over gRPC and uses a well defined ProtocolBuffer Message Service interface:</p>"},{"location":"architecture/processors/srp/overview/#time-window-aggregation-capability","title":"Time-Window Aggregation Capability","text":"<p>For simple workloads the SRP Processor offers lighter, configurable tumbling time-window capabilities with in-memory or local state persistence.  A tumbling time-window assigns each message to a window of a specified time interval. Tumbling windows have a fixed size and do not overlap.  Find detailed description of SRP Time-Window capabilities.</p> <p></p>"},{"location":"architecture/processors/srp/overview/#streaming-data-partitioning","title":"Streaming Data Partitioning","text":"<p>Common Streaming Data Partitioning Support for SRP and SCS Processor types.</p> <p></p>"},{"location":"architecture/processors/srp/time-window-aggregation/","title":"Time-Window Aggregation","text":"<p>Some data transformations, such as group-by-key, aggregate multiple messages by a common key. For bounded (e.g. batch) datasets, those operations group all of the messages with the same key within the entire data set. But it is impossible in unbounded datasets (e.g. streaming), where new records are being ingested infinitely.  Consequently, such types of data workloads are commonly processed in windows. Any unbounded dataset can be divided into logical windows. Each message received from the ubound datasets is assigned to one or more windows according to some windowing function, ensuring that each individual window contains a finite number of messages. Grouping transformations then can be applied on each message on a per-window basis. For example group-by-key groups the records within a time-window.</p> <p>The Streaming Runtime offers two approaches for data windowing. For demanding, complex data processing, you should consider applying the FSQL Processor type, which provides integration with the powerful Apache Flink Streaming SQL. The FSQL samples demonstrate how to leverage this approach. </p> <p>For simple workload the SRP Processor offers lighter, configurable <code>tumbling time-window</code> capabilities with in-memory or local state persistence.  A tumbling time-window assigns each message to a window of a specified time interval. Tumbling windows have a fixed size and do not overlap. For example, if you specify a tumbling window with a size of 5 minutes.</p> <p>This feature is useful for workloads where you need to calculate aggregates continuously. For example, for a retailer streaming order information from different remote stores, it can generate near real-time sales statistics for downstream processing. </p> <p>The SRP Processor\u2019s time-window aggregates functionality is commonly used in combination with Aggregate UDFs that operate on the aggregated window data.  When enabled the tumbling window functionality groups the inbound messages in windowed aggregates and sends later to the configured, Aggregation UDF. The UDF function returns a value or multiple values that are sent downstream for further processing.</p> <p>To enable the tumbling window, you need to set the <code>window duration</code> with the SRP Processor <code>srp.window attribute</code>. This instructs the processor to collect the inbound messages into time-window groups based on a <code>event-time</code> computed for every message. The inbound Stream\u2019s <code>spec.timeAttributes</code> defines how the message event-time is computed and via the watermark expression, how to generate the input watermarks.</p> <p>Even-Time vs Processing Time</p> <p>When processing data which relate to events in time, there are two inherent domains of time to consider: (1) Event Time, which is the time at which the event itself actually occurred and (2) Processing Time, which is the time at which an event is observed at any given point during processing within the pipeline, i.e. the current time according to the system clock. Event time for a given event essentially never changes, but processing time changes constantly for each event as it flows through the pipeline and time marches ever forward. This is an important distinction when it comes to robustly analysing events in the context of when they occurred.</p> <p>The Streaming Runtime can compute event-times from the inbound message body or metadata (e.g. Kafka timestamp header) and defaults to processing time if no time attributes are specified.</p> <p>The <code>Watermarks</code>, implemented by the <code>SRP Processor</code>, is an essential tool for reasoning about temporal completeness in infinite streams. It provides a practical way to estimate when a time-window aggregate should have received all its messages and can be sent to the UDF. In essence the watermark toolkit helps to propagate consistently the event-time (e.g. the time event occurred in its business domain) as the input events (e.g. messages) get processed through the streaming pipeline (at processing time).</p> <p>For example, let's construct a simple streaming analytics pipeline for a multi-platform team game, aggregating every 15 seconds, per-team scores.  Let\u2019s assume that the input data has a format like this: </p> <pre><code>User &lt;name: string, team: (red or blue), score:int, time_score: timestamp&gt;\n</code></pre> <p>Then we can configure the SR pipeline to use the <code>time_score</code> field as event-time and a watermark to tolerate a few seconds out-of-orderness. Then set the processor with <code>srp.window</code> interval of <code>15 seconds</code> and references to the polyglot <code>Aggregation UDF</code> that computes the results from the window aggregates.  For example we can implement a simple Node.js UDF that groups the <code>scores</code> per <code>team</code> in every window. </p> <p>The <code>Stream</code> and <code>Processor</code> definitions for this example might look like this:</p> <p></p> <p>You can find here the actual time-window aggregation example data-pipeline: 6-time-window-aggregation.yaml</p> <p>Without getting into details about how the watermarks work internally, here is an approximation how the inbound messages are grouped in time-window aggregates, which after processed by the UDF produces new outbound messages sent downstream:</p> <p></p> <p>When deployed the pipeline would look like this:</p> <p></p>"},{"location":"architecture/processors/srp/time-window-aggregation/#partitioned-time-window-aggregation","title":"Partitioned Time-window Aggregation","text":"<p>The time-window aggregation collects, locally, temporal groups of messages (e.g. windows) that could put pressure on Pod\u2019s memory. Also the aggregation tasks are often CPU demanding and can become a performance bottleneck. To alleviate those problems we can use the processor replications property to spin multiple Processor instances that do the aggregation in parallel. Because this is an aggregation Process the SR operator would make sure to run the new instances into stateful (aka SatefulSet) containers. But just increasing the number of Processor instances would likely lead to incorrect aggregation results. Since the aggregation task performs group-by-key it is critical that the same key for a given time-window interval is always sent to the same Processor instance!</p> <p>Fortunately Data Partitioning provides this capability! If we partition the inbound Stream on the same key used for grouping we can ensure that the message middleware and Kubernetes  state management will always dispatch the messages with the same keys to the same processor instance.</p> <p>In short we can reliably scale out the time-window aggregation processors by ensuring inbound data partitioning on the same key! So our scaled out multiplatform-game application will look like this:</p> <p></p> <p>Explore related examples:</p> <ul> <li>6.1 Partition by Field with replicated Time-Window aggregation</li> <li>Online Gaming Statistics</li> </ul>"},{"location":"architecture/processors/srp/udf-overview/","title":"User Defined Functions","text":"<p>The Streaming Runtime provides a pluggable User Defined Functions (UDF) that allows implementing the streaming transformation logic in a language of your choice, test it in isolation with your favorite tools and finally  package it in a standalone container image.</p> <p>The function is deployed as a sidecar along the SRP processor that acts as the connection between the streams and the function deployed. To build your custom function it should adhere to the UDF Protocol Buffer Contract and run as gRPC service.</p>"},{"location":"architecture/processors/srp/udf-overview/#udf-types","title":"UDF Types","text":"<p>Two types of UDF functions are supported: <code>Mapping UDF</code> and <code>Aggregation UDF</code></p> <p> Mapping UDF - The SRP processor forwards the inbound messages, element-wise over the <code>MessagingService</code>, to the UDF function. </p> <p>The function uses the input to compute a result and return it to the SRP.  In turn the SRP wraps the result as an internal message and sends it downstream. </p> <p>Every inbound message produces a single outbound result!  </p> <p> Aggregation UDF - When the <code>Time-Window</code> aggregation is enabled and a window is ready for release, the SRP processor forwards the window content (e.g. collection of messages) to the UDF function. Later processes the collection,  computes one or more aggregation results that are returned to the SRP and sent downstream.</p> <p>Every aggregation could produces one or more outbound results!  </p> <p>The Mapping and the Aggregating UDF container images are registered with the SRP Processor CR, using the the <code>spec.templates.spec.containers</code> configuration section.  The SR uses this information to run the UDF image as a side-container in the same pod as the SRP Processor.</p>"},{"location":"architecture/processors/srp/udf-overview/#resource-definition","title":"Resource Definition","text":"<p>To plug a custom UDF to your SRP Processor, you can refer to UDF\u2019s image from within the Processor resourced definition: </p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata: {}\nspec:\n# Processor Type: Streaming Runtime Processor (SRP)\ntype: SRP\n# Name of the input stream to get data from\ninputs:\n- name: &lt;string&gt;\n# Name of the output stream to send data to\noutputs:\n- name: &lt;string&gt;\nattributes:\n# UDF gRPC connection port\nsrp.grpcPort: \"50051\"\ntemplate:\nspec:\ncontainers:\n# Container with the UDF function image\n- name: &lt;your-udf-container-name&gt;\nimage: &lt;udf-repository-uri&gt;\n# Environment variables applied to the UDF at runtime\nenv:\n- name: &lt;string&gt;\nvalue: &lt;any&gt;\n</code></pre>"},{"location":"architecture/processors/srp/udf-overview/#udf-contract","title":"UDF Contract","text":"<p>The contract of the function specifies a <code>GrpcMessage</code> schema to model the messages exchanged between the multibinder and the function and the <code>MessagingService</code> rpc service to interact with the UDF. </p> <p>The <code>GrpcPayloadCollection</code> is a temporal workaround to help serialize/deserialize collection on messages, for example the time-window aggregates,  to and from single byte array. This allow the SRP to sends time-window aggregates to the UDFs using the same <code>GrpcMessage</code> format. </p> <pre><code>syntax = \"proto3\";\noption java_multiple_files = true;\npackage org.springframework.cloud.function.grpc;\nmessage GrpcMessage {\nbytes payload = 1;\nmap&lt;string, string&gt; headers = 2;\n}\nmessage GrpcPayloadCollection {\nrepeated bytes payload = 1;\n}\nservice MessagingService {\nrpc requestReply(GrpcMessage) returns (GrpcMessage);\n}\n</code></pre> <p>The MessageService.proto allows you to generate required stubs to support the true polyglot nature of gRPC while interacting with functions hosted by <code>Streaming Runtime</code>.</p> <p>The SRP Processor forwards the incoming messages over the <code>MessagingService</code> to the pre-configured UDF function. The function response in turn is sent to the SRP's output stream.</p>"},{"location":"architecture/processors/srp/udf-overview/#mapping-udf","title":"Mapping UDF","text":"<p>The Mapping UDF function runs a gRPC server with the <code>MessagingService</code> implementation. </p> <p>As shown in the following diagram, the SRP processor converts every inbound SR message into a <code>GrpcMessage</code> and invokes the <code>requestReply</code> method on the <code>MessagingService</code>. </p> <p></p> <p>The UDF <code>MessagingService#requestReply</code> implementation, handles the invocation, computes a result and returns it back as <code>GrpcMessage</code>. The SRP processor converts the <code>GrpcMessage</code> result into internal SR Message and sends it downstream over the outbound Streams.</p> <p>The 3.1-polyglot-udf-transformation.yaml example, uses a simple Python mapping UDF to convert the payload to upper case. Following diagram visualizes how this <code>polyglot-udf-transformation.yaml</code> example is deployed by the Streaming Runtime into a running data pipeline:</p> <p></p> <p>Processor's <code>spec.templates.spec.containers</code> properties are used to register the UDF's image with the SRP processor to use it.</p> <p>Sidecar</p> <p>The Streaming RUntime collocates the UDF container along with the SPR processor container in the same Pod.  This simplifies the (gRPC) communication between both containers as they use the 'localhost' network.</p> <p>Here a few snippets how to implement Mapping UDFs in different languages:</p> <ul> <li> <p>Java: <pre><code>public Function&lt;String, String&gt; uppercase() {\nreturn v -&gt; v.toUpperCase();\n}\n</code></pre> You can find complete source code udf-uppercase-java. If you are building your <code>Function</code> in Java you can find more information about the Spring Cloud Function gRPC support here.</p> </li> <li> <p>Python: <pre><code>def requestReply(self, request, context):\nprint(\"Server received Payload: %s and Headers: %s\" % (request.payload.decode(), request.headers))\nreturn MessageService_pb2.GrpcMessage(\npayload=str.encode(request.payload.decode().upper()), headers=request.headers)\n</code></pre> You can find complete source code udf-uppercase-python</p> </li> <li> <p>GoLang: <pre><code>func (s *server) RequestReply(ctx context.Context, in *pb.GrpcMessage) (*pb.GrpcMessage, error) {\nlog.Printf(\"Received: %v\", string(in.Payload))\nupperCasePayload := strings.ToUpper(string(in.Payload))\nreturn &amp;pb.GrpcMessage{Payload: []byte(upperCasePayload)}, nil\n}\n</code></pre> You can find complete source code udf-uppercase-go</p> </li> </ul>"},{"location":"architecture/processors/srp/udf-overview/#aggregation-udf","title":"Aggregation UDF","text":"<p>When the <code>time-window</code> aggregation is used the <code>SRP</code> processor forwards to the UDF not just a single message but the collection of all messages members of a time-window aggregation. Reversely the UDF may return not just a single result but a collection of results that are treated as separate downstream messages.</p> <p>The <code>MessagingService</code>, used by the <code>Mapping UDFs</code>, expects a single <code>GrpcMessage</code> as input and single <code>GrpcMessage</code> as an output. So if we are to reuse the same gRPC service for <code>Aggregation UDFs</code> we need a workaround to allow serializing and deserializing collection of <code>SR Messages</code> to and from single <code>GrpcMessage</code>. Furthermore we need to do it in interoperable (e.g. language neutral) fashion.  </p> <p>The <code>GrpcPayloadCollection</code> message format is used to ensure interoperability of serialization and deserialization of the payloads for the messages exchanged between the SR Processor and the Aggregation UDF.  Following diagram illustrates the message flow:</p> <p></p> <p>The <code>SR Message</code> collections (aka time-window) is converted into a single <code>GrpcMessage</code>.  The headers of the first SR Message in the window is used as headers for the GrpcMessage, including a hardcoded <code>contentType</code> header of type <code>multipart/&lt;inner-message-content-type&gt;</code>. All SR Message payloads in the window are serialized, with the GrpcPayloadCollection help, into a single byte array used as GrpcMessage payload.</p> <p>The <code>Aggregation UDF</code> is required to deserialize the GrpcMessage payload back into a collection of the original payloads, then apply the aggregation transformation and serialize the collection or results into a single byte array passed as payload in the return GrpcMessage.</p> <p>Finally the SRP Processor turns the returned GrpcMessage into collection of SR Messages and sends them down streams, one by one.</p> <p>The udf-utilities offers some helpers library that help to hide the gRPC and SerDeser boilerplate code.</p> <p>Note</p> <p>The <code>GrpcPayloadCollection</code> serialization/deserialization approach is a hackish workaround to reuse the existing <code>MessagingService</code> applicable for non-aggregated messages exchange.  A proper, cleaner approach would be to implement a dedicated <code>AggregatedMessagingService</code> that takes a collection of <code>GrpcMessage</code> messages as input and output.</p> <p>Check the Time-Window Aggregation to see how Aggregation UDFs are being used to compute group-by-key results.</p>"},{"location":"architecture/processors/srp/udf-build/udf-build/","title":"Build UDFs","text":""},{"location":"architecture/processors/srp/udf-build/udf-build/#build-mapping-udf","title":"Build Mapping UDF","text":"<p>The udf-uppercase-java, udf-uppercase-go, and udf-uppercase-python sample projects show how to build simple UDFs in <code>Java</code>, <code>Python</code> or <code>Go</code> using the <code>Reques/Repply</code> RPC mode. Also, you can find there instructions how to build the UDF container image and push those to the container registry of choice.</p> <p>For example in case of the Python UDF you can use a <code>Dockerfile</code> like this:</p> <pre><code>FROM python:3.9.7-slim\nRUN pip install grpcio\nRUN pip install grpcio-tools\nADD MessageService_pb2.py /\nADD MessageService_pb2_grpc.py /\nADD message_service_server.py /\nENTRYPOINT [\"python\",\"/message_service_server.py\"]\nCMD []\n</code></pre> <p>to build the container image: <pre><code>docker build -t ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1 .\n</code></pre> and push it to the registry: <pre><code>docker push ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1\n</code></pre></p> <p>Then you can refer this image from within your streaming <code>Processor</code> CR definitions.  For example:</p> <pre><code>1.  apiVersion: streaming.tanzu.vmware.com/v1alpha1\n2.  kind: Processor\n3.  metadata:\n4.    name: my-streaming-processor\n5.  spec:\n6.    inputs: 7.      - name: \"my-input-stream\" # input streams for the UDF function  \n8.    outputs: 9.      - name: \"my-output-stream\" # output streams for the UDF function        \n10.   template:\n11.     spec:\n12.       containers:\n13.         - name: my-python-udf-container\n14.           image: ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1\n</code></pre> <p>Note that the <code>my-python-udf-container</code> (lines <code>13</code>-<code>14</code>) uses the <code>udf-uppercase-python:0.1</code> image.</p> <p>Next: learn how to build Aggregate UDF</p>"},{"location":"architecture/processors/srp/udf-build/udf-build/#build-aggregation-udf","title":"Build Aggregation UDF","text":"<p>The gaming-team-score, gaming-user-score, and fraud-detection-udf-js sample projects show how to build simple UDFs in <code>Java</code> or <code>NodeJS/JavaScript</code>.</p> <p>Also, you can find there instructions how to build the UDF container image and push those to the container registry of choice.</p> <p>Above projects use internally the udf-utilities that help to hide most of the gRPC boilerplate code. Currently the udf-utils provides support for Java and NodeJS though it can be extended for the other programming languages as well. </p> <p>For example lets implement an <code>Aggregation UDF</code> in NodeJS/JavaScript that computes the scores by team name in in a temporal aggregate (aka time-window):</p> <ul> <li>As prerequisite you need <code>node</code> and <code>npm</code> installed.</li> <li>Create a new folder <code>team-scores</code> and an empty <code>aggregate.js</code> file inside.</li> <li>Add <code>package.json</code> to your project. Mind the <code>streaming-runtime-udf-aggregator</code> dependency. package.json<pre><code>{\n\"name\": \"gaming-team-score\",\n\"version\": \"1.0.0\",\n\"description\": \"\",\n\"main\": \"aggregate.js\",\n\"scripts\": {\n\"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\"\n},\n\"author\": \"\",\n\"license\": \"ISC\",\n\"dependencies\": {\n\"streaming-runtime-udf-aggregator\": \"^1.0.6\"\n}\n}\n</code></pre></li> <li>Run <code>npm install</code> to install required <code>node_modules</code>.</li> <li> <p>Then implement your <code>aggregate.js</code> aggregate.js<pre><code>const udf = require('streaming-runtime-udf-aggregator');\n// --------- UDF aggregation function --------\nfunction aggregate(headers, user, results) {\nif (!results.has(user.team)) {\n// Add new empty team aggregate to the result map\nresults.set(user.team, {\nfrom: headers.windowStartTime,\nto: headers.windowEndTime,\nteam: user.team,\ntotalScore: 0,\n});\n}\n// Increment team's score.\nlet team = results.get(user.team);\nteam.totalScore =\nNumber.parseInt(team.totalScore) + Number.parseInt(user.score);\n}\nnew udf.Aggregator(aggregate).start();\n</code></pre> On line (1), we include the stream-aggregate npm library, that adds the streaming-runtime-udf-aggregator-js helpers to your implementation. Line (23) uses the helper to instantiate and <code>Aggregator</code>, that internally starts a gRPC server and registers and <code>aggregation</code> functions. When new time-window aggregation is received the Aggregator deserializes its content into a collection of ordered messages and for every message in the aggregation calls the <code>aggregator</code> function. The Aggregator also creates and maintains an result key/value state that is passed to and updated by the aggregation function. On lines <code>4-21</code> we implement the aggregation function.  Later creates new result entry for every unique <code>user.team</code> and computes the score sum for it.  After all messages in the time-window aggregate are processed by the aggregation function, the Aggragator helper serializes the results and returns them (over the gRPC contract) to the SRP Processor.</p> </li> <li> <p>Finally create and publish a docker image for the UDF implementation. First create a Dockerfile look like this: Dockerfile<pre><code># syntax=docker/dockerfile:1\nFROM node:18.4.0\nENV NODE_ENV=production\n\nWORKDIR /app\nCOPY [\"package.json\", \"package-lock.json*\", \"./\"]\nRUN npm install --production\n\nCOPY aggregate.js .\n\nCMD [ \"node\", \"aggregate.js\" ]\n</code></pre> and use ist to build the image: <pre><code>docker build --tag ghcr.io/vmware-tanzu/streaming-runtimes/team-score-js .\n</code></pre> and publish it <pre><code>docker push ghcr.io/vmware-tanzu/streaming-runtimes/team-score-js:latest\n</code></pre> (Note: you can use the tags and container registries of your choice)</p> </li> </ul> <p>Once published the UDF image can be used from withing your <code>SRP Processor</code> CR definition.</p> <p>Here is the full project on GitHub: team-score-aggregation-js.</p>"},{"location":"architecture/service-binding/service-binding/","title":"Service Binding","text":""},{"location":"architecture/service-binding/service-binding/#overview","title":"Overview","text":"<p>To fulfil its tasks, the Streaming Runtime (SR) interacts with external distributed systems such as Apache Kafka, RabbitMQ, Apache Flink and others.  This implies that SR uses credentials to access those systems.  Furthermore the SR internally exchanges status information between the managed (<code>ClusterStream</code>, <code>Stream</code> and <code>Processor</code>) resources and part of this information could include sensitive attributes as well. To avoid configuring and sharing sensitive attributes in plain text the SR needs a mechanism to encapsulate and securely share such attributes amongst its managed resources. </p> <p>The Service Binding Specification for Kubernetes is designed to address this problem by providing a Kubernetes-wide specification for communicating service secrets to workloads in an automated way.</p> <p></p> <p>The <code>ClusterStream</code> CRD offers a dedicated storage.server.binding attribute that can be used to refer to an existing Service Binding Service (e.g. secrets).</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: ClusterStream\nmetadata:\nname: test-clusterstream\nspec:\nname: my-exchange\nstorage:\nserver:\nbinding: \"my-service-binding-ref\"\nurl: \"http://localhost:8080\"\nprotocol: \"rabbitmq\"     </code></pre> <p>The <code>ClusterStream</code> reconciler will detect this attribute and converted it into <code>status.binding.name</code> in compliance with the ProvisioningService specification.</p> <pre><code>...\nstatus:\nbinding:\nname: \"my-service-binding-ref\"\n...    </code></pre> <p>Later is picked by the <code>Service Binding Operator</code>. It lookups and enforces all <code>ServiceBinding</code> resources with service name matching the provide binding name. For example</p> <pre><code>apiVersion: servicebinding.io/v1beta1\nkind: ServiceBinding\nmetadata:\nname: streaming-runtime-rabbitmq\nspec:\nservice:\napiVersion: v1\nkind: Secret\nname: my-service-binding-ref\nworkload:\napiVersion: apps/v1\nkind: Deployment\nname: streaming-runtime-processor-possible-fraud-processor\n</code></pre>"},{"location":"architecture/service-binding/service-binding/#enable-service-binding","title":"Enable Service Binding","text":"<ul> <li> <p>Install the Service Binding Operator. Any specification compliant operator can be used but we advice for the VMWare-Tanzu operator: <pre><code>kubectl apply -f https://github.com/vmware-tanzu/servicebinding/releases/download/v0.7.1/service-bindings-0.7.1.yaml\n</code></pre></p> </li> <li> <p>Create Kubernetes Secrets for the protected services (e.g. Kafka, RabbitMQ \u2026).  Note: When operators are used to provision those services, later create the needed secrets automatically. Follow the service operator instructions to find the names of the generated secrets.</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: streaming-runtime-rabbitmq-secret\ntype: servicebinding.io/rabbitmq\nstringData:\ntype: rabbitmq\nprovider: rabbitmq\nhost: rabbitmq.default.svc.cluster.local\nport: \"5672\"\n# demo credentials\nusername: guest\npassword: guest\n</code></pre> <ul> <li>Create <code>Service Binding</code> contracts to bind the desired services and workloads.</li> </ul> <pre><code>apiVersion: servicebinding.io/v1beta1\nkind: ServiceBinding\nmetadata:\nname: streaming-runtime-rabbitmq\nspec:\nservice:\napiVersion: v1\nkind: Secret\nname: streaming-runtime-rabbitmq-secret\nworkload:\napiVersion: apps/v1\nkind: Deployment\nname: streaming-runtime-processor-possible-fraud-processor\nenv:\n- name: SPRING_RABBITMQ_PASSWORD\nkey: password\n- name: SPRING_RABBITMQ_USERNAME\nkey: username\n</code></pre> <ul> <li>Add binding attribute to <code>Stream</code> resource to refer the service secret name</li> </ul> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: udf-output-possible-fraud-stream\nspec:\nname: udf-output-possible-fraud\nkeys: [ \"card_id\" ]\nstreamMode: [ \"write\" ]\nprotocol: \"rabbitmq\"\n# Binding refers to a Secret with the same name. The stream controller uses this binding to configure ClusterStream's auto-creation\nbinding: \"streaming-runtime-rabbitmq-secret\"\nstorage:\nclusterStream: \"udf-output-possible-fraud-cluster-stream\"\n</code></pre> <p>Future Work</p> <p>The Streaming Runtime should be able to create and manage the ServiceBinding objects internally</p>"},{"location":"architecture/streams/overview/","title":"Streams","text":"<p>The Streams CRD represents storage-at-rest of time-ordered attribute-partitioned data, such as a Kafka topic, or RabbitMQ Stream, exchange/queue.</p> <p>The <code>Stream</code> represents the binder/channel between two or more Processors.</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata: {}\nspec:\n# Name of the Stream\nname: &lt;string&gt;\n# (Optional) Data Schema and Time Attributes\ndataSchemaContext:\ninline:\nSchema: &lt;inline-schema&gt;  #(or \nschema: &lt;meta-schema&gt; )\n...\ntimeAttributes:\n- name: ...\nwatermark: ...\n# Attributes used as partitioning keys (either keys or keyExpression is allowed)\nkeys: [ &lt;string&gt; ]\n# Attributes used as partitioning keys (either keys or keyExpression is allowed)\nkeyExpression: [ &lt;string&gt; ]\n# Stream mode that the stream will be used for\nstreamMode: [ &lt;string&gt; ]\n# Protocol to be used for the stream e.g. kafka\nprotocol: &lt;string&gt;\n# (optional) Binding refers to a Secret with the same name. \n# The stream controller uses this binding to configure ClusterStream's auto-creation.\nbinding: &lt;string&gt;\nstorage:\n# Name of the ClusterStream resource\nclusterStream: &lt;string&gt;\n</code></pre> <p>For a detailed description of attributes of the resource please read stream-crd.yaml</p> <p>The namespaced Streams declared (created) by a developer are backed by a ClusterStream resource which is controlled and provisioned by the administrator.</p>"},{"location":"architecture/streams/overview/#clusterstream-relationship","title":"ClusterStream relationship","text":"<p>The ClusterStreams and the <code>Streams</code> follow the PersistentVolume model: namespaced <code>Stream</code> declared by a developer (ala <code>PVC</code>) is backed by a <code>ClusterStream</code> resource (ala <code>PV</code>) which is controlled and provisioned by the administrator. For convenience during the development stage, the SR operator auto-provisions the <code>ClusterStreams</code> for all <code>Streams</code> that don't have explicitly declared them.</p>"},{"location":"architecture/streams/overview/#data-schema","title":"Data Schema","text":"<p>The Stream CRDs can provide an elaborate Data Schema to define the structure, time and serialization details of the Stream messages.  The data schema context comprises a <code>schema</code> of the message payload along with additional <code>time-attributes</code>, metadata mappings and configuration options. </p> <p></p>"},{"location":"architecture/streams/overview/#data-partitioning","title":"Data Partitioning","text":"<p>For SRP and SCS processor types the <code>key</code> and <code>keyExpression</code> attributes are used to configure a data partitioning of the streamed data.</p> <p></p>"},{"location":"architecture/streams/overview/#service-binding","title":"Service Binding","text":"<p>The Service Binding Specification provides a Kubernetes-wide specification for communicating service secrets to workloads in an automated way. The Stream <code>spec.binding</code> allow to refer existing service binding service (aka secrets).</p>"},{"location":"architecture/streams/overview/#data-policies","title":"Data Policies","text":"<p>WIP</p>"},{"location":"architecture/streams/overview/#usage","title":"Usage","text":"<p>Streams can be used as either input or output in the stream processing system;</p> <p></p> <p>When deployed each Stream resource is represented by a messaging middleware topic/queue/exchange.</p> <p>Similarly the Processor resources are represented by executable message processing processors that adhere to the Streaming Runtime platform requirements.</p>"},{"location":"architecture/streams/streaming-data-schema/","title":"Stream Data Schema","text":"<p>The Stream CRDs can provide a data schema context to define the structure, time and serialization details of the messages they represent. The data schema context comprises a schema of the message payload along with additional time-attributes, metadata mappings and configuration options. The data schema describes the structure of the message payload. For a convenience, several, semantically equivalent, schema representations are supported. The time attributes augment can assign process or event time to schema in order to support streaming data processing. The metadata fields can extend the schema with additional, \u201csynthetic\u201d,  fields that are extracted or computed from the message\u2019s metadata. The options allow specifying some configuration details, such as serialization, encoding formats, or properties passed directly through the backend sql aggregation engines.</p> <p>Following diagram illustrates the relationships between the Data Schema Context parts:</p> <p></p>"},{"location":"architecture/streams/streaming-data-schema/#schema-payload-data-model","title":"Schema (Payload Data Model)","text":"<p>The schema describes the structure of the message payload. It can be expressed as an Avro Schema, standard SQL/DDL or using the custom Stream MetaSchema (defined in the CRD and validated as OpenAPISchema). Additionally the payload schema can be retrieved from a remote schema registry such as Confluent Schema Registry using the Avro schema.</p> <p>Following snippets show the same data structure using the different schema representations.</p> Meta-SchemaInline Apache AvroInline SQL <pre><code>namespace: test.ns\nname: PlayEvents\nfields:\n- name: duration\ntype: long\noptional: true\n- name: event_time\ntype: long_timestamp-millis\nmetadata:\nfrom: timestamp\nreadonly: true\nwatermark: \"`event_time`- INTERVAL '30' SECONDS\"\n</code></pre> <pre><code>{\n\"type\" : \"record\",\n\"name\" : \"PlayEvents\",\n\"namespace\" : \"test.ns\",\n\"fields\" : [{\n\"name\" : \"duration\",\n\"type\" : [\"null\", \"long\"]\n}]\n}\n</code></pre> <pre><code>CREATE TABLE PlayEvents (\n`duration` BIGINT\n)\n</code></pre> <p>Note how the Meta-Schema representation extends the data schema content with <code>metadata</code> and <code>time-attributes</code> information. For the other representations this information is applied in the outer Schema Data Context sections (see the examples below).</p>"},{"location":"architecture/streams/streaming-data-schema/#time-attributes","title":"Time Attributes","text":"<p>Streaming data processing can process data based on different notions of time. Processing time refers to the machine\u2019s system time that is executing the respective operation. Event time refers to the processing of streaming data based on timestamps that are attached to each row. The timestamps can encode when an event has happened.</p> <p>Time attributes can be part of every Stream\u2019s data schema. They are defined when creating the Stream CR. Once a time attribute is defined, it can be referenced as a field in Processor\u2019s queries and used in time-based operations.</p>"},{"location":"architecture/streams/streaming-data-schema/#event-time","title":"Event Time","text":"<p>Event time allows a Processor query to produce results based on timestamps in every message, allowing for consistent results despite out-of-order or late events. It also ensures the replayability of the results of the streaming pipeline when reading messages from persistent storage (such as Kafka).</p> All representationsMeta-Schema only <pre><code>dataSchemaContext:\ninline:\nSchema: &lt;inline-schema&gt;  #(or \nschema: &lt;meta-schema&gt; )\n...\ntimeAttributes:\n- name: event_time\nwatermark: \"`event_time` - INTERVAL '30' SECONDS\"\n</code></pre> <pre><code>dataSchemaContext:\nschema:\nnamespace: \u2026\nname: Songs\nfields:\n#...\n- name: event_time\ntype: long_timestamp-millis\nwatermark: \"`event_time` - INTERVAL '30' SECONDS\"\n</code></pre> <p>The general syntax for defining Event Time field, assumes adding a timeAttributes entry only with name and watermark expression! (note without or empty watermark stands for Process Time field). You can mix Meta-Schema and general definitions. The general syntax precedes.</p>"},{"location":"architecture/streams/streaming-data-schema/#processing-time","title":"Processing Time","text":"<p>Processing time allows the streaming processing to produce results based on the time of the local machine. It is the simplest notion of time, but it will generate non-deterministic results. Processing time does not require timestamp extraction or watermark generation.</p> <p>There are two ways to define a processing time attribute.</p> All representationsMeta-Schema only <pre><code>dataSchemaContext:\ninline:\nSchema: &lt;inline-schema&gt;    # (or   \nschema: &lt;meta-schema&gt; )\n...\ntimeAttributes:\n- name: my_proctime_field\n</code></pre> <pre><code>dataSchemaContext:\nschema:\nnamespace: \u2026\nname: Songs\nfields:\n#...\n- name: my_proctime_field\ntype: proctime\n</code></pre> <p>The general syntax for defining Process Time field, assumes adding a timeAttributes entry only with name but without watermark! You can mix Meta-Schema and general definitions. In case of conflict the general syntax precedes.</p>"},{"location":"architecture/streams/streaming-data-schema/#metadata","title":"Metadata","text":"<p>Represents a special class of schema fields that are inferred or computed from the message\u2019s metadata. For example the message\u2019s timestamp or headers can be used as schema fields.</p> All representationsMeta-Schema only <pre><code>dataSchemaContext:\ninline:\nSchema: &lt;inline-schema&gt;    # (or   \nschema: &lt;meta-schema&gt; )\n...\nmetadataFields:\n- name: event_time\ntype: long_timestamp-millis\nmetadata:\nfrom: timestamp\nreadonly: true\n</code></pre> <pre><code>dataSchemaContext:\nschema:\nnamespace: \u2026\nname: Songs\nfields:\n#...       \n- name: event_time\ntype: long_timestamp-millis\nmetadata:\nfrom: timestamp\nreadonly: true\n</code></pre>"},{"location":"architecture/streams/streaming-data-schema/#options","title":"Options","text":"<p>Various serialization/deserialization configurations, remote system connection or implementation optimizations. The streaming runtime will try to hide as much as possible those details from the end user by inferring them from the Stream or ClusterStream statuses or assume some reasonable defaults for the common use cases. Yet the end user can use the Options section to further configure/optimize or override the defaults.</p>"},{"location":"architecture/streams/streaming-data-schema/#primary-key","title":"Primary Key","text":"<p>Primary key constraint is a hint for Streaming processing to leverage for optimizations. It indicates that a field or a set of fields of a data schema are unique and they do not contain null. Neither of the fields in a primary key can be nullable.</p> <pre><code>dataSchemaContext:\ninline:\nSchema: &lt;inline-schema&gt;\n#(or\nschema: &lt;meta-schema&gt; )\n\u2026\nprimaryKey: [ \"song_id\", \"genre\" ]\n</code></pre>"},{"location":"architecture/streams/streaming-data-schema/#schema-formats-description","title":"Schema Formats Description","text":""},{"location":"architecture/streams/streaming-data-schema/#meta-schema-format","title":"Meta-Schema Format","text":"<p>Informal description of the Meta-Schema format.</p> YAML Default Description <code>namespace: &lt;my.name.space&gt;</code> Schema namespace. <code>name: &lt;schema-name&gt;</code> Schema name unique in the namespace. <code>fields.name: &lt;field-name&gt;</code> Schema name unique in the namespace. <code>fields.type: &lt;field-type&gt;</code> Same as Avro primitive types  plus one additional type: <code>proctime</code>. Later indicates that this is a Process time field. Also you can use the shortcut format <code>&lt;type&gt;_&lt;logicalType&gt;</code> to set both the type and the logical type attributes of the field. <code>fields.logicalType: &lt;filed-logical-type&gt;</code> Same as Avro\u2019s logicalTypes. <code>fields.optional: &lt;true or false&gt;</code> false If this field is optional in the schema. <code>metadata.from: &lt;message metadata&gt;</code> field.name Name of the massage metadata attributes to be used as value of the metadata field. This is dependent on the type of the input. For example for Kafka we can use the message timestamp as a metadata field. <code>metadata.readonly: &lt;true/false &gt;</code> true Whenever this metadata field can be overridden when sending a new message to the binder. <code>watermark: &lt;expression&gt;</code> true The watermark expression. It indicates that the field used in the expression is an Event-Time field."},{"location":"architecture/streams/streaming-data-schema/#inline-sql-format","title":"Inline-SQL Format","text":"<p>Uses standard ANSI SQL CREATE TABLE statements to define the data schema. The schema definition may look like this:</p> <pre><code> CREATE TABLE SongPlays (\n`song_id` BIGINT NOT NULL,\n`album` STRING,\n`genre` STRING NOT NULL,\n`duration` BIGINT,\n`event_time` TIMESTAMP(3) NOT NULL\n)\n</code></pre> <ul> <li>Table name stands for Schema name.</li> <li>Table column names represent the schema field names.</li> <li>The table column types (standard SQL types) represent the field types.</li> <li>The \u201cNOT NULL\u201d type constraint indicates if the schema field is optional or not.</li> <li>The column types used as Time Attribute MUST use the TIMESTAMP(3) type.</li> </ul> <p>No metadata or time attributes fields are defined in this schema.</p>"},{"location":"architecture/streams/streaming-data-schema/#inline-avro-format","title":"Inline-Avro Format","text":"<p>Follows the Avro 1.11.0 schema specification. Uses the JSON representation.</p> <pre><code>{\n\"type\" : \"record\",\n\"name\" : \"SongPlays\",\n\"namespace\" : \"net.tzolov.poc.playsongs.avro\",\n\"fields\" : [{\n\"name\" : \"song_id\",\n\"type\" : \"long\"\n}, {\n\"name\" : \"album\",\n\"type\" : [ \"null\", \"string\" ]\n}, {\n\"name\" : \"artist\",\n\"type\" : [ \"null\", \"string\" ]\n}, {\n\"name\" : \"name\",\n\"type\" : [ \"null\", \"string\" ]\n}, {\n\"name\" : \"genre\",\n\"type\" : \"string\"\n}, {\n\"name\" : \"duration\",\n\"type\" : [ \"null\", \"long\" ]\n}, {\n\"name\" : \"event_time\",\n\"type\" : {\n\"type\" : \"long\",\n\"logicalType\" : \"timestamp-millis\"\n}\n}]\n}\n</code></pre> <p>Note the same format is used for the schemas stored in Schema-Registries.</p> <p>(N.B. Streaming runtime converts internally all different representations into Avro schema).</p>"},{"location":"architecture/streams/streaming-data-schema/#examples","title":"Examples","text":"<p>Here is an example payload schema represented using the Meta-Schema, Inline-Avro, Inline-SQL representation.</p>"},{"location":"architecture/streams/streaming-data-schema/#meta-schema","title":"Meta-Schema","text":"<pre><code>dataSchemaContext:\nschema:\nnamespace: net.tzolov.poc.playsongs.avro\nname: PlayEvents\nfields:\n- name: song_id\ntype: long\n- name: duration\ntype: long\noptional: true\n- name: event_time\ntype: long_timestamp-millis\nmetadata:\nfrom: timestamp\nreadonly: true\nwatermark: \"`event_time`- INTERVAL '30' SECONDS\"\noptions:\nddl.scan.startup.mode: earliest-offset\n</code></pre>"},{"location":"architecture/streams/streaming-data-schema/#inline-avro","title":"Inline-Avro","text":"<pre><code>dataSchemaContext:\ninline:\ntype: avro\nschema: |\n{\n\"type\" : \"record\",\n\"name\" : \"PlayEvents\",\n\"namespace\" : \"net.tzolov.poc.playsongs.avro\",\n\"fields\" : [ {\n\"name\" : \"song_id\",\n\"type\" : \"long\"\n}, {\n\"name\" : \"duration\",\n\"type\" : [ \"null\", \"long\" ]\n} ]\n}\nmetadataFields:\n- name: event_time\ntype: long_timestamp-millis\nmetadata:\nfrom: timestamp\nreadonly: true\ntimeAttributes:\n- name: event_time\nwatermark: \"`event_time` - INTERVAL '30' SECONDS\"\noptions:\nddl.scan.startup.mode: earliest-offset\n</code></pre> <p>Note that the Schema Registry referenced schema are special case of the Inline-Avro:</p> <pre><code>dataSchemaContext:\ninline:\ntype: avro-confluent\nschema:&lt;schema-registry-url&gt;/\u200b\u200bsubjects/&lt;topic&gt;-value/versions/latest\nmetadataFields:\n- name: event_time\ntype: long_timestamp-millis\nmetadata:\nfrom: timestamp\nreadonly: true\ntimeAttributes:\n- name: event_time\nwatermark: \"`event_time` - INTERVAL '30' SECONDS\"\noptions:\nddl.scan.startup.mode: earliest-offset\n</code></pre> <p>If only the type: avro-confluent attributed is provided, with empty schema value then the streaming runtime will default the schema value to:  /\u200b\u200bsubjects/-value/versions/latest, assuming that the schema registry URL is provided via the Options."},{"location":"architecture/streams/streaming-data-schema/#inline-sql","title":"Inline-SQL","text":"<p>(using a plain ANSI compliant SQL)</p> <pre><code>dataSchemaContext:\ninline:\ntype: sql\nschema: |\nCREATE TABLE PlayEvents (\n`song_id` BIGINT NOT NULL,\n`duration` BIGINT\n)\nmetadataFields:\n- name: event_time\ntype: long_timestamp-millis\nmetadata:\nfrom: timestamp\nreadonly: true\ntimeAttributes:\n- name: event_time\nwatermark: \"`event_time` - INTERVAL '30' SECONDS\"\noptions:\nddl.scan.startup.mode: earliest-offset\n</code></pre> <p>The above three data schema representations are semantically identical and for each of them the Processor will generate exactly the same executable physical schema. In case of Apache Flink, the processor will generate the following CREATE TABLE DDL:</p> <pre><code>CREATE TABLE PlayEvents (\n`song_id` BIGINT NOT NULL,\n`duration` BIGINT,\n`the_kafka_key` STRING,\n`event_time`TIMESTAMP(3) NOT NULL METADATA FROM 'timestamp' VIRTUAL,\nWATERMARK FOR `event_time` AS `event_time` - INTERVAL '30' SECONDS\n) WITH (\n'properties.bootstrap.servers' = 'kafka.default.svc.cluster.local:9092',\n'connector' = 'kafka',\n'value.format' = 'avro-confluent',\n'key.format' = 'raw',\n'value.fields-include' = 'EXCEPT_KEY',\n'topic' = 'kafka-stream-playevents',\n'value.avro-confluent.url' = 'http://s-registry.default.svc.cluster.local:8081',\n'key.fields' = 'the_kafka_key' )\n</code></pre> <p>Note how the metadata and the time attributes are mapped. Also most of the options (in the WITH sections) are inferred from the Stream\u2019s status server address and some defaults.</p>"},{"location":"samples/instructions/","title":"How to Run","text":"<p>All use-cases are organized in folders named of after the use-case, each containing two files:</p> <pre><code>streaming-runtime-samples/\n    &lt;use-case-folder&gt;/\n        streaming-pipeline.yaml \n        data-generator.yaml\n</code></pre> <ul> <li><code>streaming-pipeline.yaml</code> is a manifest of Streaming-Runtime custom resources, such as <code>ClusterStream</code>, <code>Stream</code> and <code>Processor</code>, defining the use-case data processing pipeline.</li> <li><code>data-generator.yaml</code> manifest deploys the Data Generator that continuously generates realistic test data for this particular use case. </li> </ul>"},{"location":"samples/instructions/#run-a-sample","title":"Run a Sample","text":"<p>Follow the Streaming Runtime installation instructions to deploy the operator.</p> <p>Next from within the <code>streaming-runtime-samples</code> directory, deploy the use-case streaming pipeline:</p> <pre><code>kubectl apply -f '&lt;use-case-folder&gt;/streaming-pipeline.yaml' -n streaming-runtime\n</code></pre> <p>and the data generator to provide test data for this use case: <pre><code>kubectl apply -f '&lt;use-case-folder&gt;/data-generator.yaml' -n streaming-runtime\n</code></pre></p> <p>Note</p> <p>Substitute the <code>&lt;use-case-folder&gt;</code> placeholder with the folder name of the use-case of choice.</p>"},{"location":"samples/instructions/#explore-the-results","title":"Explore the Results","text":"<p>All input and output streams are backed by messaging systems such as Apache Kafka or RabbitMQ and we can explore the messages exchanged through the pipeline.  </p>"},{"location":"samples/instructions/#explore-apache-kafka-topics","title":"Explore Apache Kafka Topics","text":""},{"location":"samples/instructions/#using-kowl-ui","title":"Using Kowl UI","text":"<p>The auto-provisioned Apache Kafka clusters come pre-configured with the Apache Kowl UI visualization tool. To access it you need to forward the <code>80</code> port first: <pre><code>kubectl port-forward svc/kafka-kowl-ui 8082:80 -n streaming-runtime\n</code></pre></p> <p>Then open http://localhost:8082/topics or http://localhost:8082/schema-registry</p> TpicsTpics DetailsSchema Registry <p></p> <p></p> <p></p>"},{"location":"samples/instructions/#using-command-line","title":"Using Command Line","text":"<p>Use the <code>kubectl get all</code> to find the Kafka broker pod name and then <pre><code>kubectl exec -it pod/&lt;your-kafka-pod&gt; -- /bin/bash`\n</code></pre> to SSH to kafka broker container.</p> <p>From within the kafka-broker container use the bin utils to list the topics or check their content:</p> <pre><code>/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n</code></pre> <p>Then to list the topic content: <pre><code>/opt/kafka/bin/kafka-console-consumer.sh --topic &lt;topic-name&gt; --from-beginning --bootstrap-server localhost:9092\n</code></pre></p> <p>To delete a topic: <pre><code>/opt/kafka/bin/kafka-topics.sh --delete --topic &lt;topic-name&gt; --bootstrap-server localhost:9092\n</code></pre></p>"},{"location":"samples/instructions/#rabbit-queues","title":"Rabbit Queues","text":"<p>To access the Rabbit management UI first forward the <code>15672</code> port: <pre><code>kubectl port-forward svc/rabbitmq 15672:15672 -n streaming-runtime\n</code></pre></p> <ol> <li>Then open http://localhost:15672/#/exchanges and find the exchange name related to your use-case.</li> <li>Open the <code>Queues</code> tab and create new queue called <code>myTempQueue</code> (use the default configuration).</li> <li>Go back to the <code>Exchang</code> tab, select the use-case exchange and bind it to the new <code>myTempQueue</code> queue, with <code>#</code> as a <code>Routing key</code>!</li> <li>From the <code>Queue</code> tab select the <code>myTempQueue</code> queue and click the <code>Get Messages</code> button.</li> </ol> <p></p>"},{"location":"samples/overview/","title":"Overview","text":""},{"location":"samples/overview/#tutorials","title":"Tutorials","text":"<p>Step by step tutorials introduce the Streaming Runtime features and how to use them.</p>"},{"location":"samples/overview/#use-cases","title":"Use Cases","text":"<p>Example use cases below demonstrate how to implement various streaming and event-driven use case scenarios with the help of the <code>Streaming Runtime</code>.</p> <p>The setup instructions helps to setup the demo infrastructure (e.g. minikube) and to explore the demo results - e.g. exploring the Apache Kafka topics and/or RabbitMQ queues data.</p> <ul> <li> Anomaly Detection - FSQL (FSQL, SRP)- detect, in real time, suspicious credit card transactions, and extract them for further processing.</li> <li> Anomaly Detection - SRP (SRP, CSC)- detect, in real time, suspicious credit card transactions, and extract them for further processing.</li> <li> Clickstream Analysis (FSQL, SRP) -   for an input clickstream stream, we want to know who are the high status customers, currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day.</li> <li> IoT Monitoring analysis (FSQL, SRP) - real-time analysis of IoT monitoring log.</li> <li> Streaming Music Service (FSQL, SRP) - music ranking application that continuously computes the latest Top-K music charts based on song play events collected in real-time.</li> <li>Spring Cloud Stream pipeline (SCS) - show how to build streaming pipelines using Spring Cloud Stream application as processors.</li> <li>Online Game Statistics (SRP+UDF) - WIP</li> </ul>"},{"location":"samples/tutorials/","title":"Tutorials","text":"<p>The step by step tutorials introduce the SR features and how to use them.</p> <p>All tutorials snippets are located under the tutorials folder.</p>"},{"location":"samples/tutorials/#quick-start","title":"Quick start","text":"<ul> <li> <p>Follow the install instructions to instal the <code>Streaming Runtime</code> operator.</p> </li> <li> <p>Deploy a selected tutorial data pipeline.  Replace the <code>&lt;select-tutorial-pipeline&gt;</code> with one of the sinppet file names form the tutorials folder. <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/tutorial/&lt;select-tutorial-pipeline&gt;.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Run the test data generator. Later generates random data, sent to the <code>data-in</code> Kafka topic: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/tutorials/data-generator.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. </p> </li> <li> <p>To delete the data pipeline and the data generator: <pre><code>kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app=tutorial-data-generator -n streaming-runtime\n</code></pre></p> </li> </ul>"},{"location":"samples/tutorials/#1-message-retransmission","title":"1. Message Retransmission","text":"<p>Processor re-transmits, unchanged, the events/messages received from the input (data-in) Stream into the output (data-out) Stream. The data-in and data-out Stream (CRD) resources are auto-provisioned using the runtime operator defaults, such a Kafka as a default protocol. The Stream resources, in turn, auto-provision their ClusterStreams (CRD) applying the '-cluster-stream' naming convention. Finally the ClusterStream controllers will provision the required brokers for the target protocols (e.g. Kafka, RabbitMQ...).  <p>Currently 3 processor types are supported (if omitted is defaults to SRP): </p> <ul> <li>SRP (default) - time-windowed, side-car UDF processor.</li> <li>SCS - Spring Cloud Stream/Function processor.</li> <li>FSQL - Apache Flink (inline) streaming SQL processor.</li> </ul> <p>One can combine multiple different processor types in the same data pipelines. 1. Message Retransmission<pre><code># 1. Message Retransmission\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: re-transmission-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in\noutputs:\n- name: data-out\n</code></pre></p>"},{"location":"samples/tutorials/#2-multibinder-bridge","title":"2. Multibinder Bridge","text":"<p>Configure the Stream resources explicitly. The <code>spe.protocol</code> instructs what broker to be provisioned for this Stream.  Current runtime implementation is using only Apache Kafka and RabbitMQ, but it can easily extended to support those additional Binders (e.g. message stream brokers):</p> <ul> <li>RabbitMQ</li> <li>Apache Kafka</li> <li>Amazon Kinesis</li> <li>Google PubSub</li> <li>Solace PubSub+</li> <li>Azure Event Hubs</li> <li>Azure Service Bus Queue Binder</li> <li>Azure Service Bus Topic Binder</li> <li>Apache RocketMQ</li> </ul> <p>The Stream resource represent a Binder-Access-Request to the ClusterStream resource that provisions it. When the referred ClusterStream resource is not defined the Stream reconcile Controller will try to auto-provision a ClusterStreams (unless this behavior is disabled).  It is the ClusterStream that provisions the required Binders for the target protocols (e.g. Kafka, RabbitMQ...). Different protocol deployment options are available. It defaults to built-in protocol adapters but can be configured to use operators such as RabbitOperator, Strimzi or alike instead! 2. Multibinder Bridge<pre><code># 2. Multibinder (e.g. multi-message brokers) Bridge\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-in-stream\nspec:\nname: data-in\nprotocol: \"kafka\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: multibinder-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in-stream\noutputs:\n- name: data-out-stream\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-out-stream\nspec:\nname: data-out\nprotocol: \"rabbitmq\"\n# attributes:\n#   protocolAdapterName: \"rabbitmq-operator\"\n#\n# Prerequisites to provision RabbitMQ clusters with the the \"rabbitmq-operator\":\n# https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-rabbitmq-cluster-and-message-topology-operators\n#\n</code></pre></p>"},{"location":"samples/tutorials/#21-multibinder-bridge-production-env","title":"2.1 Multibinder Bridge - production env","text":"<p>In production environment the Streaming Runtime will not be allowed to auto-provision the messaging brokers dynamically.  Instead the Administrator will provision the required messaging middleware and declare ClusterStream to provide managed and controlled access to it.</p> <p>The ClusterStreams and the Streams follow the PersistentVolume model:  namespaced Stream declared by a developer (ala PVC) is backed by a ClusterStream resource (ala PV) which is controlled and provisioned by the administrator. 2.1 Multibinder Bridge - production env<pre><code># 2.1 Multibinder Bridge - production env\n#################################################\n#  ADMIN responsibility\n#################################################\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: ClusterStream\nmetadata:\nname: kafka-cluster-stream\nspec:\nname: data-in\nstreamModes: [\"read\", \"write\"] # Note enforced yet\nstorage:\nserver:\nurl: \"kafka.default.svc.cluster.local:9092\"\nprotocol: \"kafka\"\nreclaimPolicy: \"Retain\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: ClusterStream\nmetadata:\nname: rabbitmq-cluster-stream\nspec:\nname: data-out\nstreamModes: [\"read\", \"write\"]\nstorage:\nserver:\nurl: \"rabbitmq.default.svc.cluster.local:5672\"\nprotocol: \"rabbitmq\"\nreclaimPolicy: \"Retain\"\n---\n#################################################\n#  DEVELOPER responsibility\n#################################################\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-in-stream\nspec:\nname: data-in\nprotocol: \"kafka\"\nstorage:\nclusterStream: kafka-cluster-stream # Claims the pre-provisioned Kafka ClusterStream.\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: multibinder-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in-stream\noutputs:\n- name: data-out-stream\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-out-stream\nspec:\nname: data-out\nprotocol: \"rabbitmq\"\nstorage:\nclusterStream: rabbitmq-cluster-stream # Claims the pre-provisioned rabbitmq ClusterStream.\n</code></pre></p>"},{"location":"samples/tutorials/#3-inline-data-transformation","title":"3. Inline Data Transformation","text":"<p>The SpEL expressions can be applied to transform the input payloads on the fly. </p> <ul> <li>The spel.expression is applied on the inbound message and the result is used as outbound payload.</li> <li>The output.headers expression extracts values from the inbound headers/payload and injects new key/value headers to the outbound messages: =&lt;[payload.|header.]expression&gt; <p>Note: SRP specific only. 3. Inline (e.g. in SRP Processor) Data Transformation<pre><code># 3. Inline (e.g. in SRP Processor) Data Transformation \napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: inline-transformation\nspec:\ntype: SRP\ninputs:\n- name: data-in\noutputs:\n- name: data-out\nattributes:\nsrp.output.headers: \"user=payload.fullName\"  srp.spel.expression: '''{\"'' + #jsonPath(payload, ''$.fullName'') + ''\":\"'' + #jsonPath(payload, ''$.email'') + ''\"}'''\n</code></pre></p>"},{"location":"samples/tutorials/#31-polyglot-udf-transformation","title":"3.1 Polyglot UDF Transformation","text":"<p>The (SRP) Processor can be assigned with custom User Defined Function running in a sidecar container next to the processor in the same Pod. Processor calls the UDF either for every received message or in the case of temporal aggregation calls it once the aggregate is ready. </p> <p>The communication between the Processor and the custom UDF is performed over gRPC using well defined Protocol Buffer contract.  Because the Protocol Buffers are language-neutral, this allows implementing the UDF in any language of choice! (e.g. polyglot UDF).</p> <p>Detailed UDF documentation: https://vmware-tanzu.github.io/streaming-runtimes/architecture/processors/srp/udf-overview/</p> <p>Note: The inline transformations can be applied on the outbound message (e.g. the UDF response) before it is sent.</p> <p>Note: SRP specific feature. 3.1 Polyglot UDF Transformation<pre><code># 3.1 Polyglot UDF Transformation\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: udf-transformation\nspec:\ntype: SRP\ninputs:\n- name: data-in\noutputs:\n- name: data-out\nattributes:\nsrp.grpcPort: \"50051\"\ntemplate:\nspec:\ncontainers:\n- name: uppercase-grpc-python\n# Runs GRPC server on port 50051\nimage: ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-python:0.1\n</code></pre></p>"},{"location":"samples/tutorials/#32-scs-spring-cloud-stream-transformation","title":"3.2  SCS (Spring Cloud Stream) Transformation","text":"<p>Any Spring Cloud Stream or Spring Cloud Function application can be run as Processor. Just build a container image for the application and run it as `spec.type: SCS`` Processor type.</p> <p>Spring Cloud DataFlow provides 60+ pre-built SCS/SCF applications that can be used Out-Of-The-Box</p> <p>Use the environment variables to configure the Spring application.</p> 3.2  SCS (Spring Cloud Stream) Transformation<pre><code># 3.2  SCS (Spring Cloud Stream) Transformation\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: time-source\nspec:\ntype: SCS\noutputs:\n- name: timestamps-stream\ntemplate:\nspec:\ncontainers:\n- name: scdf-time-source-kafka\nimage: springcloudstream/time-source-kafka:3.2.0\nenv:\n- name: SPRING_CLOUD_STREAM_POLLER_FIXED-DELAY\nvalue: \"2000\"\n- name: TIME_DATE-FORMAT\nvalue: \"dd/MM/yyyy HH:mm:ss\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: log-sink\nspec:\ntype: SCS\ninputs:\n- name: timestamps-stream\ntemplate:\nspec:\ncontainers:\n- name: scdf-log-sink-kafka\nimage: springcloudstream/log-sink-kafka:3.2.0\nenv:\n- name: LOG_EXPRESSION\nvalue: \"'My uppercase timestamp is: ' + payload\"\n</code></pre>"},{"location":"samples/tutorials/#4-stateless-replication","title":"4. Stateless Replication","text":"<p>By default the Processor controller deploys one instance for every Processor. The <code>spec.replicas</code> is used to set the desired number of processor instances.</p> <p>For not-partitioned input the runtime creates, stateless, Kubernetes Deployment Pods for every processor instance and configures round-robing message delivery policy.  Every inbound message is deliver to ONLY one processor instance selected on round-robing principle. The order of the instances is not guarantied.</p> <p>In case of partitioned input or processor 'forceStatefulSet=true' attribute, the runtime operator creates StatefulSet Pods with strict guarantees about the ordering and uniqueness of these Pods. Unlike a Deployment, the StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p> <p>If you want to use storage volumes to provide persistence for your workload, or use Stream partitioning, then <code>StatefulSet</code> is default configuration. Although individual Pods in a <code>StatefulSet</code> are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed. 4. Stateless Replication<pre><code># 4. Stateless Replication\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: stateless-replication\nspec:\ntype: SRP\n# 3 instances\nreplicas: 3\ninputs:\n- name: data-in\noutputs:\n- name: data-out\n# attributes:\n#   forceStatefulSet: \"true\"\n</code></pre></p>"},{"location":"samples/tutorials/#5-partition-by-field-with-stateful-replication","title":"5. Partition by Field with Stateful Replication","text":"<p>Processor types: <code>SRP</code>, <code>SCS</code></p> <p>Documentation: Data Partitioning </p> <p>On the Steam resource that represents the partitioned connection, use the <code>spec.keyExpression</code> to define the what header or payload field to use as a discriminator to partition the data in the steam.  Additionally use the spec.partitionCount property to configure the number of partitions you would like the incoming data to be partitioned into.  Those properties are used to instruct the upstream processor(s) to provision the data partitioning configuration while the downstream processors are configured for partitioned inputs (e.g. enforce instance ordering and stateful connections). </p> <p>If the downstream processor is scaled out (e.g. <code>replications: N</code>), then the streaming runtime will ensure <code>StatefulSet</code> replication instead of <code>Deployment</code>/<code>ReplicationSet</code>. Additionally, for the processors consuming partitioned Stream, the SR configures Pod's Ordinal Index to be used as partition instance-index.  Later ensures that event after Pod failure/restart the same partitions will be (re)assigned to it.</p> 5. Partition by Field with Stateful Replication<pre><code># 5. Partition by Field with Stateful Replication\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-in-stream\nspec:\nname: data-in\nprotocol: kafka\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: team-partition-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in-stream\noutputs:\n- name: partitioned-by-team-stream\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: partitioned-by-team-stream\nspec:\nname: partitioned-by-team\nprotocol: kafka\nkeyExpression: \"payload.team\"\npartitionCount: 3\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: team-scores-processor\nspec:\ntype: SRP\nreplicas: 3\ninputs:\n- name: partitioned-by-team-stream\noutputs:\n- name: team-scores-stream\nattributes:\nsrp.spel.expression: \"'Team:' + #jsonPath(payload, '$.team') + ', Score:' + #jsonPath(payload, '$.score')\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: team-scores-stream\nspec:\nname: team-scores\nprotocol: kafka\n</code></pre>"},{"location":"samples/tutorials/#51-partition-by-field-using-header-keys","title":"5.1 Partition by Field using Header Keys","text":"<p>Variation of the 5-partition-by-field-with-stateful-replication.yaml that uses message headers as partitioning keys.  The 'spec.keys' value in the partitioned Stream must exist as a header name in the messages carried by that stream.</p> <p>Also the 'data-in-stream' and 'team-scores-stream' Stream definitions are dropped in favor of auto-provisioned defaults.</p> 5.1 Partition by Field with Stateful Replication (Header Keys)<pre><code># 5.1 Partition by Field using Header Keys\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: user-partition-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in\noutputs:\n- name: partitioned-by-team-stream\nattributes:\n# The header name used for partitioning must match the outbound stream's spec.keys names!!!\nsrp.output.headers: \"team=payload.team\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: partitioned-by-team-stream\nspec:\nname: partitioned-by-team\nprotocol: kafka\n# The 'team' is expected to be a inbound message header name!!!\nkeys: [\"team\"]\npartitionCount: 3\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: user-scores-processor\nspec:\ntype: SRP\nreplicas: 3\ninputs:\n- name: partitioned-by-team-stream\noutputs:\n- name: team-scores\nattributes:\nsrp.spel.expression: \"'Team:' + #jsonPath(payload, '$.team') + ', Score:' + #jsonPath(payload, '$.score')\"\n</code></pre>"},{"location":"samples/tutorials/#52-partition-by-field-rabbitmq-version","title":"5.2 Partition by Field - RabbitMQ version","text":"<p>Same as (5.) but with RabbitMQ brokers instead for the partitioning section of the pipeline.</p> 5.2 Partition by Field with Stateful Replication (RabbitMQ)<pre><code># 5.2 Partition by Field - RabbitMQ version\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-in-stream\nspec:\nname: data-in\nprotocol: kafka\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: user-partition-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in-stream\noutputs:\n- name: partitioned-by-team-stream\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: partitioned-by-team-stream\nspec:\nname: partitioned-by-team\nprotocol: rabbitmq\nkeyExpression: \"payload.team\"\npartitionCount: 3\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: user-scores-processor\nspec:\ntype: SRP\nreplicas: 3\ninputs:\n- name: partitioned-by-team-stream\noutputs:\n- name: team-scores-stream\nattributes:\nsrp.spel.expression: '''Team:'' + #jsonPath(payload, ''$.team'') + '', Score:'' + #jsonPath(payload, ''$.score'')'\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: team-scores-stream\nspec:\nname: team-scores\nprotocol: rabbitmq\n</code></pre>"},{"location":"samples/tutorials/#6-tumbling-time-window-aggregation","title":"6. Tumbling Time-Window Aggregation","text":"<p>The (SRP) processor supports Tumbling Time-Window Aggregation.  The 'srp.window' attribute defines the window interval.  The processor collects inbound messages into time-window groups based on the event-time computed for every message. The event-time is computed from message's payload or header metadata.  The inbound Stream 'spec.dataSchemaContext.timeAttributes' defines which payload field (or header attribute) to be used as an Event-Time.  Furthermore the Watermark expression allows configuring out-of-orderness. When no event-time is configured the processor defaults to the less reliable process-time as event-time.</p> <p>Complete Tumbling Time-Window  documentation</p> 6. Tumbling Time-Window Aggregation<pre><code># 6. Tumbling Time-Window Aggregation\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-in-stream\nspec:\nname: data-in\nprotocol: kafka\ndataSchemaContext:\n# Here the schema is used only with descriptive purpose! It is not used to validate the \n# stream content. \n# Check the FSQL samples to find how to enforce Avro schemas and use schema registries.\nschema:\nnamespace: sr.poc.online.gaming\nname: User\nfields:\n- name: id\ntype: string\n- name: fullName\ntype: string\n- name: team\ntype: string\n- name: email\ntype: string\n- name: score\ntype: int\n- name: score_time\ntype: long_timestamp-millis\ntimeAttributes:\n# Data field to be used as an event-time. Generated watermark uses 2 sec. out-of-orderness.\n# Note: The Out-Of-Order events are not Late Events! The LateEvents are handled differently.\n- name: score_time\nwatermark: \"`score_time` - INTERVAL '2' SECOND\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: user-scores-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in-stream\noutputs:\n- name: user-scores-stream\nattributes:\nsrp.window: 5s # Tumbling Time Window of 5 seconds.\nsrp.window.idle.timeout: 60s # Allow partial release of idle time-windows.\nsrp.lateEventMode: SIDE_CHANNEL # Send late events a side-channel stream. By default late events are discarded.\ntemplate:\nspec:\ncontainers:\n- name: scores-by-user-javascript\n# The UDF implementation is in the './6-user-score-aggregation-js/aggregate.js'\nimage: ghcr.io/vmware-tanzu/streaming-runtimes/user-score-js:latest\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: user-scores-stream\nspec:\nname: user-scores\nprotocol: kafka\n</code></pre> ghcr.io/vmware-tanzu/streaming-runtimes/user-score-js:latest<pre><code>const udf = require('streaming-runtime-udf-aggregator');\n// --------- UDF aggregation function --------\nfunction aggregate(headers, user, results) {\nif (!results.has(user.fullName)) {\n// Add new empty user aggregate to the result map\nresults.set(user.fullName, {\nfrom: headers.windowStartTime,\nto: headers.windowEndTime,\nname: user.fullName,\ntotalScore: 0,\n});\n}\n// Increment user's score.\nlet userAggregate = results.get(user.fullName);\nuserAggregate.totalScore =\nNumber.parseInt(userAggregate.totalScore) + Number.parseInt(user.score);\n}\nnew udf.Aggregator(aggregate).start();\n</code></pre>"},{"location":"samples/tutorials/#61-partition-by-field-with-replicated-time-window-aggregation","title":"6.1 Partition by Field with replicated Time-Window aggregation","text":"<p>Reliably scale out the time-window aggregation processors by ensuring inbound data partitioning on the same key.</p> <p>Complete documentation</p> 6.1 Partition by Field with replicated Time-Window aggregation<pre><code># 6.1 Partition by Field with replicated Time-Window aggregation\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-in-stream\nspec:\nname: data-in\nprotocol: kafka\ndataSchemaContext:\nschema:\nnamespace: sr.poc.online.gaming\nname: User\nfields:\n- name: id\ntype: string\n- name: fullName\ntype: string\n- name: team\ntype: string\n- name: email\ntype: string\n- name: score\ntype: int\n- name: score_time\ntype: long_timestamp-millis\ntimeAttributes:\n# Data field to be used as an event-time.\n# Generated watermark uses 2 sec. out-of-orderness.\n- name: score_time\nwatermark: \"`score_time` - INTERVAL '2' SECOND\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: team-partition-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in-stream\noutputs:\n- name: partitioned-by-team-stream\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: partitioned-by-team-stream\nspec:\nname: partitioned-by-team\nprotocol: kafka\n# Partition by Team name\nkeyExpression: \"payload.team\"\n# Break into 3 partition groups\npartitionCount: 3\n# Note: The inbound message event-time is injected automatically by the upstream processor into the 'header.eventtime'.\n# The downstream processor looks it up automatically, no need to define it explicitly via the Stream 'timeAttributes'.\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: team-scores-processor\nspec:\ntype: SRP\n# Process the scores in parallel -\n# one processor instance per partition.\nreplicas: 3 inputs:\n- name: partitioned-by-team-stream\noutputs:\n- name: team-scores-stream\nattributes:\nsrp.window: 5s\nsrp.window.idle.timeout: 140s\ntemplate:\nspec:\ncontainers:\n- name: scores-by-team-javascript\nimage: ghcr.io/vmware-tanzu/streaming-runtimes/team-score-js:latest\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: team-scores-stream\nspec:\nname: team-scores\nprotocol: kafka\n</code></pre> ghcr.io/vmware-tanzu/streaming-runtimes/team-score-js:latest<pre><code>const udf = require('streaming-runtime-udf-aggregator');\n// --------- UDF aggregation function --------\nfunction aggregate(headers, user, results) {\nif (!results.has(user.team)) {\n// Add new empty team aggregate to the result map\nresults.set(user.team, {\nfrom: headers.windowStartTime,\nto: headers.windowEndTime,\nteam: user.team,\ntotalScore: 0,\n});\n}\n// Increment team's score.\nlet team = results.get(user.team);\nteam.totalScore =\nNumber.parseInt(team.totalScore) + Number.parseInt(user.score);\n}\nnew udf.Aggregator(aggregate).start();\n</code></pre>"},{"location":"samples/tutorials/#7-fsql-processor-examples","title":"7. FSQL processor examples","text":"<ul> <li>Anomaly Detection (FSQL, SRP)- detect, in real time, suspicious credit card transactions, and extract them for further processing.</li> <li>Clickstream Analysis (FSQL, SRP) -   for an input clickstream stream, we want to know who are the high status customers, currently using the website so that we can engage with them or to find how much they buy or how long they stay on the site that day.</li> <li>IoT Monitoring analysis (FSQL, SRP) - real-time analysis of IoT monitoring log.</li> <li>Streaming Music Service (FSQL, SRP) - music ranking application that continuously computes the latest Top-K music charts based on song play events collected in real-time.</li> </ul>"},{"location":"samples/tutorials/#8-secretes-management-with-service-binding-spec","title":"8. Secretes Management with Service Binding spec","text":"<p>We can use the RabbitMQ Cluster Operator to provision our RabbitMQ Cluster and the ServiceBinding Operator to share the RabbitMQ credentials with the processors that would need to connect to it.</p> <p>Prerequisites:</p> <ul> <li>Service Binding Operator: https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-service-binding-operator</li> <li>RabbitMQ Cluster and Message Topology Operators: https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-rabbitmq-cluster-and-message-topology-operators</li> </ul> <p>Complete service binding documentation 8. Secretes Management with Service Binding spec<pre><code># 8. Secretes Management with Service Binding spec.\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-in-stream\nspec:\nname: data-in\nprotocol: \"kafka\"\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: multibinder-processor\nspec:\ntype: SRP\ninputs:\n- name: data-in-stream\noutputs:\n- name: data-out-stream\n---\napiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: data-out-stream\nspec:\nname: data-out\nprotocol: \"rabbitmq\"\n# Binding refs a Secret with same name. The stream controller uses this binding to configure ClusterStream's auto-creation\nbinding: \"data-out-stream-cluster-stream-default-user\"\nattributes:\n# Prerequisites to provision RabbitMQ clusters with the the \"rabbitmq-operator\":\n# https://vmware-tanzu.github.io/streaming-runtimes/install/#optional-install-rabbitmq-cluster-and-message-topology-operators\nprotocolAdapterName: \"rabbitmq-operator\"\n---\napiVersion: servicebinding.io/v1beta1\nkind: ServiceBinding\nmetadata:\nname: streaming-runtime-rabbitmq\nspec:\nservice:\napiVersion: v1\nkind: Secret\n# This is the secret generated by the RabbitMQ Cluster operator. Convention is:\n# '&lt;cluster-stream-name&gt;-default-user'\n# When the ClusterStream is automatically generated from the Stream Operator, the name convention is:\n# '&lt;stream-name&gt;-cluster-stream-default-user'\nname: data-out-stream-cluster-stream-default-user\nworkload:\napiVersion: apps/v1\nkind: Deployment\n# Currently, the convention expects that the workload name is: 'srp-&lt;your-processor-name&gt;'.\n# TODO: Explore generating the ServiceBindings resources in the  StreamingRuntime Operator.\nname: srp-multibinder-processor\nenv:\n# As we know that this processor uses Spring RabbitMQ configuration, here we pass in the expected conf.\n- name: SPRING_RABBITMQ_PASSWORD\nkey: password\n- name: SPRING_RABBITMQ_USERNAME\nkey: username\n</code></pre></p>"},{"location":"samples/anomaly-detection/anomaly-detection-srp/","title":"Credit Card Anomaly Detection - SRP","text":"<p>This is the same Use Case as Anomaly Detection (FSQL) but the time-window aggregation is implemented with the SRP Processor instead of the FSQL one.</p> <p>Imagine a stream of credit card authorization attempts, representing, for example, people swiping their chip cards into a reader or typing their number into a website. </p> <p>Such stream may look something like this:</p> <pre><code>{\"card_number\": \"1212-1221-1121-1234\", \"card_type\": \"discover\", \"card_expiry\": \"2013-9-12\", \"name\": \"Mr. Chester Stracke\" },\n{\"card_number\": \"1234-2121-1221-1211\", \"card_type\": \"dankort\", \"card_expiry\": \"2012-11-12\", \"name\": \"Preston Abbott\" }\n...\n</code></pre> <p>We would like to identify the suspicious transactions, in real time, and extract them for further investigations.  For example we can count the incoming authorization attempts per card number and identify those authorizations that occurs suspiciously often.</p> <p>Lets use the <code>Streams</code> and <code>Processors</code> streaming-runtime resources to build such abnormal authorization detection application:</p> <p></p> <p>The input stream, <code>card-authorization</code> , does not provide a time field for the time when the authorization attempt was performed.  Such field would have been preferred option for the time widowing grouping. The next best thing is to use the message timestamp assigned by the message broker to each message. The implementation details section below explain how this is done to provision an additional <code>event_time</code> field to the authorization attempts data schema.</p> <p>The <code>possible-fraud-detection</code> CR definition leverages the Time-Window Aggregation and User Defined Function capabilities provided by the SRP Processor type:</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: possible-fraud-detection\nspec:\ntype: SRP\ninputs:\n- name: card-authorizations-stream\noutputs:\n- name: fraud-alert-stream\nattributes:\nsrp.window: 5s\nsrp.window.idle.timeout: 60s\ntemplate:\nspec:\ncontainers:\n- name: fraud-detection-udf\nimage: ghcr.io/vmware-tanzu/streaming-runtimes/udf-anomaly-detection-js:latest\n</code></pre> <p>We are only interested in frequent authorization attempts that happen in short intervals of time.  For this we split the incoming stream into a series of fixed-sized, non-overlapping and contiguous time intervals called <code>Tumbling Windows</code> (<code>12-13</code>).  Here we aggregate the stream in intervals of <code>5 seconds</code> assuming that <code>5</code> authorization attempts in <code>5</code> seconds would be hard for a person to do.  Swiping the card or submitting the form five times within five seconds is a little weird. </p> <p>To group the incoming authorization attempts by the card numbers we use an Aggregation UDF function. (registered at lines: <code>17-18</code>). </p> <p>The <code>udf-anomaly-detection-js</code> UDF is implemented in Node.js - full code is here and published as container image.</p> <p>The actual aggregation code looks like this:</p> fraud-detector.js<pre><code>const udf = require('streaming-runtime-udf-aggregator');\n// --------- UDF aggregation function --------\nfunction aggregate(headers, authorization, results) {\nif (!results.has(authorization.card_number)) {\nresults.set(authorization.card_number, {\nfrom: headers.windowStartTime,\nto: headers.windowEndTime,\ncard_number: authorization.card_number,\ncount: 0,\n});\n}\n// Increment the authorization count for that card_number.\nlet authorizationCounter = results.get(authorization.card_number);\nauthorizationCounter.count = Number.parseInt(authorizationCounter.count) + 1;\n}\n// --------- UDF release results function --------\nfunction release(results) {\nlet finalResults = new Map();\n// Filter in only the aggregates with more than 5 authorization attempts.    \nresults.forEach((authorizationCounter, card_number) =&gt; {\nif (authorizationCounter.count &gt; 5) {\nfinalResults.set(card_number, authorizationCounter);\n}\n});\nreturn finalResults;\n}\nnew udf.Aggregator(aggregate, release).start();\n</code></pre> <p>The <code>Aggregator</code> (line <code>36</code>) is a helper that starts the gRPC server and listens for next time window aggregate collected and sent from the SRP Processor. When new aggregate arrives the <code>Aggregator</code> calls the <code>aggregate</code> function (line <code>4</code>) sequentially for every entry in the aggregate.  The function (lines <code>4-19</code>) implements a group-by-card_number count and stores it in the shared <code>result</code> object.</p> <p>When all entries in the window are processed by the <code>aggregate</code> function, the <code>Aggregator</code> sends the computed results to the <code>release</code> function.  Later filters in only the result counts larger than <code>5</code>. Finally the filtered results are send back to the SRP processor and each result entry is converted into a separate message send downstream.</p> <p>Next the <code>fraud-alert</code> CSC processor is used to print the fraudulent results. </p>"},{"location":"samples/anomaly-detection/anomaly-detection-srp/#quick-start","title":"Quick start","text":"<ul> <li> <p>Follow the Streaming Runtime Install instructions to instal the operator.</p> </li> <li> <p>Install the anomaly detection streaming application: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/light/streaming-pipeline-light.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Install the authorization attempts random data stream: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/light/data-generator-light.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. </p> </li> <li> <p>To delete the data pipeline and the data generator: <pre><code>kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app=authorization-attempts-data-generator -n streaming-runtime\n</code></pre></p> </li> </ul>"},{"location":"samples/anomaly-detection/anomaly-detection-srp/#next-step","title":"Next Step","text":"<p>Visit next use case: Clickstream Analysis</p>"},{"location":"samples/anomaly-detection/anomaly-detection/","title":"Credit Card Anomaly Detection - FSQL","text":"<p>Imagine a stream of credit card authorization attempts, representing, for example, people swiping their chip cards into a reader or typing their number into a website. Such stream may look something like this:</p> <pre><code>{\"card_number\": \"1212-1221-1121-1234\", \"card_type\": \"discover\", \"card_expiry\": \"2013-9-12\", \"name\": \"Mr. Chester Stracke\" },\n{\"card_number\": \"1234-2121-1221-1211\", \"card_type\": \"dankort\", \"card_expiry\": \"2012-11-12\", \"name\": \"Preston Abbott\" }\n...\n</code></pre> <p>We would like to identify the suspicious transactions, in real time, and extract them for further investigations.  For example we can count the incoming authorization attempts per card number and identify those authorizations that occurs suspiciously often.</p> <p>Lets use the <code>Streams</code> and <code>Processors</code> streaming-runtime resources to build such abnormal authorization detection application:</p> <p></p> <p>The input stream, <code>card-authorization</code> , does not provide a time field for the time when the authorization attempt was performed.  Such field would have been preferred option for the time widowing grouping. The next best thing is to use the message timestamp assigned by the message broker to each message. The implementation details section below explain how this is done to provision an additional <code>event_time</code> field to the authorization attempts data schema.</p> <p>The <code>possible-fraud-detection</code> processor leverages streaming SQL to compute the possible fraud attempts:</p> <p><pre><code> INSERT INTO [[STREAM:possible-fraud-stream]] SELECT\nwindow_start, window_end, card_number, COUNT (*) AS authorization_attempts\nFROM\nTABLE(\nTUMBLE(\nTABLE [[STREAM:card-authorizations-stream]],\nDESCRIPTOR (event_time),\nINTERVAL '5' SECONDS\n)\n)\nGROUP BY\nwindow_start, window_end, card_number\nHAVING\nCOUNT (*) &gt; 5\n</code></pre> Here we group the incoming authorization attempts by the card numbers ( lines: <code>12-13</code>) and look only at those authorizations  that have the same card number occurring suspiciously often (<code>14-15</code>).  Then we put the suspicious card numbers into a new stream (<code>1</code>).</p> <p>But it would make no sense to count throughout the entire history of the authorization attempts!  We are only interested in frequent authorization attempts that happen in short intervals of time.  For this we split the incoming stream into a series of fixed-sized, non-overlapping and contiguous time intervals called <code>Tumbling Windows</code> (<code>6-10</code>).  Here we aggregate the stream in intervals of <code>5 seconds</code> assuming that <code>5</code> authorization attempts in <code>5</code> seconds would be hard for a person to do.  Swiping the card or submitting the form five times within five seconds is a little weird.  If we see that happening it is flagged as a possible fraud and inserted to the possible-fraud-stream (<code>1</code>).</p> <p>The <code>possible-fraud-detection</code> processor emits new <code>possible-fraud</code> stream containing the fraudulent transactions.</p> <p>Next with the help for the <code>fraud-alert</code> processor we can register a custom function UDF, that consumes the <code>possible-fraud</code> stream, investigates the suspicious transactions further for example to send alert emails. </p> <p>Following diagram visualizes the streaming-pipeline.yaml, implementing the use case with <code>Stream</code> and <code>Processor</code> resources: </p>"},{"location":"samples/anomaly-detection/anomaly-detection/#quick-start","title":"Quick start","text":"<ul> <li> <p>Follow the Streaming Runtime Install instructions to instal the operator.</p> </li> <li> <p>Install the anomaly detection streaming application: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/streaming-pipeline.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Install the authorization attempts random data stream: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/anomaly-detection/data-generator.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. </p> </li> <li> <p>To delete the data pipeline and the data generator: <pre><code>kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app=authorization-attempts-data-generator -n streaming-runtime\n</code></pre></p> </li> </ul>"},{"location":"samples/anomaly-detection/anomaly-detection/#implementation-details","title":"Implementation details","text":"<p>We can implement the anomaly detection scenario with the help of the <code>Streaming Runtime</code>.  We define three <code>Streams</code> and two <code>Processor</code> custom resources.</p> <p>Note: for the purpose of the demo we will skip the explicit CusterStream definitions and instead will enable auth-provisioning for those.</p> <p>Given that the input authorization attempts stream uses an Avro data format like this:</p> <p><pre><code>{\n\"name\": \"AuthorizationAttempts\",\n\"namespace\": \"com.tanzu.streaming.runtime.anomaly.detection\",\n\"type\": \"record\",\n\"fields\": [\n{ \"name\": \"card_number\", \"type\": \"string\" },\n{ \"name\": \"card_type\", \"type\": \"string\" },\n{ \"name\": \"card_expiry\", \"type\": \"string\" },\n{ \"name\": \"name\", \"type\": \"string\" }\n]\n}\n</code></pre> We can represent it with the following custom <code>Stream</code> resource: <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: card-authorizations-stream\nspec:\nname: card-authorizations\nprotocol: \"kafka\"\nstorage:\nclusterStream: \"cluster-stream-card-authorizations\"\nstreamMode: [ \"read\" ]\nkeys: [ \"card_number\" ]\ndataSchemaContext:\nschema:\nnamespace: com.tanzu.streaming.runtime.anomaly.detection\nname: AuthorizationAttempts\nfields:\n- name: card_number\ntype: string\n- name: card_type\ntype: string\n- name: card_expiry\ntype: string\n- name: name\ntype: string\n- name: event_time\ntype: long_timestamp-millis\nmetadata:\nfrom: timestamp\nreadonly: true\nwatermark: \"`event_time` - INTERVAL '3' SECONDS\"\noptions:\nddl.scan.startup.mode: earliest-offset\n</code></pre></p> <p>The <code>event_time</code> field is auto-provisioned and assigned with Kafka message's timestamp. In addition, 3 seconds <code>watermark</code> is configured for the <code>event_time</code> field to tolerate out of order or late coming messages! </p> <p>The <code>possible-fraud-detection</code> Processor uses streaming SQL to compute the possible frauds: </p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: possible-fraud-detection\nspec:\ntype: FSQL\ninlineQuery:\n- \"INSERT INTO [[STREAM:possible-fraud-stream]]  SELECT window_start, window_end, card_number, COUNT(*) AS authorization_attempts FROM TABLE(TUMBLE(TABLE [[STREAM:card-authorizations-stream]], DESCRIPTOR(event_time), INTERVAL '5' SECONDS)) GROUP BY window_start, window_end, card_number    HAVING COUNT(*) &gt; 5\"\nattributes:\ndebugQuery: \"SELECT * FROM PossibleFraud\"\ndebugExplain: \"2\"   </code></pre> <p>Processor outputs a new <code>possible-fraud-stream</code> Stream: </p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: possible-fraud-stream\nspec:\nname: possible-fraud\nprotocol: \"kafka\"\nstorage:\nclusterStream: \"cluster-stream-possible-fraud\"\nstreamMode: [ \"read\", \"write\" ]\nkeys: [ \"card_number\" ]\ndataSchemaContext:\nschema:\nnamespace: com.tanzu.streaming.runtime.anomaly.detection\nname: PossibleFraud\nfields:\n- name: window_start\ntype: long_timestamp-millis\n- name: window_end\ntype: long_timestamp-millis\n- name: card_number\ntype: string\n- name: authorization_attempts\ntype: long\noptions:\nddl.key.fields: card_number\nddl.value.format: \"json\"\nddl.properties.allow.auto.create.topics: \"true\"\nddl.scan.startup.mode: earliest-offset\n</code></pre> <p>The <code>possible-fraud-stream</code> is given to <code>fraud-alert</code> processor configured with UDF to uppercase the payload content:</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: fraud-alert\nspec:\ntype: SRP\ninputs:\n- name: \"possible-fraud-stream\"\noutputs:\n- name: \"fraud-alert-stream\"\ntemplate:\nspec:\ncontainers:\n- name: possible-fraud-analysis-udf\nimage: ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1\n</code></pre> <p>Note that the UDF function can be implemented in any programming language.</p> <p>Finally, the output of the UDF function is send to the <code>fraud-alert-stream</code> stream defined like this: </p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: fraud-alert-stream\nspec:\nname: fraud-alert\nkeys: [ \"card_id\" ]\nstreamMode: [ \"write\" ]\nprotocol: \"rabbitmq\"\nstorage:\nclusterStream: \"cluster-stream-fraud-alert-stream\"\n</code></pre> <p>It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.</p>"},{"location":"samples/anomaly-detection/anomaly-detection/#next-step","title":"Next step","text":"<p>Check the alternative anomaly detection implementation: Anomaly Detection - SRP</p>"},{"location":"samples/clickstream/clickstream/","title":"Clickstream Analysis","text":"<p>The Streaming ETL is a common place for many people to begin first when they start exploring the streaming data processing.  With the streaming ETL we get some kind of data events coming into the streaming pipeline, and we want intelligent analysis to go out the other end.</p> <p>The clickstream analysis is common ETL data processing technique for helping understand the customer browsing behavior.</p> <p><code>Clickstream</code> data is the pathway that a user takes through their online journey.  For a single website it generally shows how the user progressed from search to purchase.  The clickstream links together the actions a single user has taken within a single session.  This means identifying where a search, click or purchase was performed within a single session.</p> <p>For example with clickstream analysis we can understand who are the high status customers currently using our websites, so that we can engage with them or find how much they buy or how long they stay on the site per day.</p> <p>Here is how the <code>Stream</code> and <code>Processor</code> resources can help us build a clickstream, data processing pipeline:</p> <p></p> <p>The first input, <code>user-stream</code>, provides detailed information about the website registered users and looks like this:</p> <pre><code>{\"user_id\":\"407-41-3862\",\"name\":\"Olympia Koss\",\"level\":\"SILVER\"}\n{\"user_id\":\"066-68-4140\",\"name\":\"Dr. Leah Daniel\",\"level\":\"GOLD\"}\n{\"user_id\":\"722-61-1415\",\"name\":\"Steven Moore\",\"level\":\"GOLD\"}\n...\n</code></pre> <p>The second input, <code>click-stream</code>, streams the actions and paths the users take throughout their website browsing journey: </p> <pre><code>{\"user_id\":\"170-65-1094\",\"page\":5535,\"action\":\"selection\",\"device\":\"computer\",\"agent\":\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.991\"}\n{\"user_id\":\"804-31-3496\",\"page\":30883,\"action\":\"checkout\",\"device\":\"tablet\",\"agent\":\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\"}\n{\"user_id\":\"011-54-8948\",\"page\":18877,\"action\":\"products\",\"device\":\"mobile\",\"agent\":\"Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1\"}\n...\n</code></pre> <p>The <code>vip-join-and-filter</code> Processor joins the input streams to enrich the click behavior with user details and filter in only the high status customer.  Processor uses streaming SQL to continuously analyze the input streams and produces a new stream containing only enriched information for the Platinum level users:</p> <pre><code>1. INSERT INTO VipActions\n2.      SELECT 3.         Users.user_id, Users.name, Clicks.page, Clicks.action, Clicks.event_time 4.      FROM 5.         Clicks\n6.      INNER JOIN 7.         Users ON Clicks.user_id = Users.user_id  8.      WHERE 9.         Users.level = 'PLATINUM'\n</code></pre> <p>Computed VIP Actions are contentiously written the the output <code>vip-action-stream</code>.</p> <p>The second, <code>vip-act-upon</code>, Processor consumes the VIP-Actions events and allows us to implement a domain specific, User Defined Function that act and apply some business logic upon the VIP events.  The UDF can be written in language of our choice!</p> <p>Following diagram visualizes the streaming-pipeline.yaml, implementing the clickstream application with the help of <code>Stream</code> and <code>Processor</code> resources: </p>"},{"location":"samples/clickstream/clickstream/#quick-start","title":"Quick start","text":"<ul> <li> <p>Follow the Streaming Runtime Install instructions to instal the operator.</p> </li> <li> <p>Install the Clickstream pipeline: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/streaming-pipeline.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Install the click-stream random data stream: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/clickstream/data-generator.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. </p> </li> <li> <p>Delete all pipelines: <pre><code>kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app=clickstream-data-generator -n streaming-runtime </code></pre></p> </li> </ul>"},{"location":"samples/clickstream/clickstream/#next-step","title":"Next step","text":"<p>Explore the IoT Monitoring use-case.</p>"},{"location":"samples/iot-monitoring/iot-monitoring/","title":"Real Time IoT Log Monitoring","text":"<p>Imagine an <code>IoT</code> network, such as network of sensors, emitting monitoring events into a central service.  We would like to analyze the incoming events for errors, and count and alert the most recent error types.</p> <p>Lets assume the input <code>iot-monitoring-stream</code> stream has a format like this:</p> <pre><code>{\"error_code\": \"C009_OUT_OF_RANGE\", \"ts\": 1645020042399, \"type\": \"ERROR\", \"application\": \"Hatity\", \"version\": \"1.16.4 \", \"description\": \"Chuck Norris can binary search unsorted data.\"}\n{\"error_code\": \"C014_UNKNOWN\", \"ts\": 1645020042400, \"type\": \"DEBUG\", \"application\": \"Mat Lam Tam\", \"version\": \"5.0.9 \", \"description\": \"Chuck Norris doesn't bug hunt, as that signifies a probability of failure. He goes bug killing.\"}\n...\n</code></pre> <p>Then we can leverage the <code>Stream</code> and <code>Processor</code> resources to build an error analysis pipeline:</p> <p></p> <p>The <code>iot-monitoring-stream</code>'s filed <code>ts</code> holds the time when the event was emitted.  Additionally a <code>watermark</code> (of <code>3</code> sec.) is configured to handle out-of-order or late coming events!</p> <p>The <code>sql-aggregator</code> processor continuously filters in the erroneous events, groups them by type and counts them over a time-window intervals. We can express processor with a streaming SQL query like this:</p> <pre><code>1.  INSERT INTO [[STREAM:error-count-stream]] 2.   SELECT\n3.     window_start, window_end, error_code, COUNT (*) AS error_count\n4.   FROM\n5.     TABLE(\n6.       TUMBLE(\n7.         TABLE [[STREAM:iot-monitoring-stream]],\n8.         DESCRIPTOR (ts),\n9.         INTERVAL '1' MINUTE\n10.      )\n11.    )\n12.    WHERE type='ERROR'\n13.    GROUP BY\n14.      window_start, window_end, error_code\n</code></pre> <p>Line (<code>12</code>) filters in only the error events. Those are grouped by <code>error_code</code> (<code>12-13</code>) to compute the counts of error events per error code. Finally a time windowing aggregation (<code>6-10</code>) is performed to compute the <code>most recent 1 minute</code> counts.</p> <p>The <code>sql-aggregation</code> processor in turn emits a stream of events, <code>error-count-stream</code>, like:</p> <pre><code>{\"window_start\":\"2022-02-16 14:18:00\",\"window_end\":\"2022-02-16 14:19:00\",\"error_code\":\"C007_INVALID_ARGUMENT\",\"error_count\":16}\n{\"window_start\":\"2022-02-16 14:18:00\",\"window_end\":\"2022-02-16 14:19:00\",\"error_code\":\"C011_RESOURCE_EXHAUSTED\",\"error_count\":28}\n{\"window_start\":\"2022-02-16 14:18:00\",\"window_end\":\"2022-02-16 14:19:00\",\"error_code\":\"C008_NOT_FOUND\",\"error_count\":28}\n</code></pre> <p>The <code>monitoring-utf</code> processor registers a User Defined Function (UDF) to post-process the computed <code>error-count-stream</code> aggregates. For example the UDF can look for the root causes of the frequently occurring error or send alerting notifications to 3rd party systems. The UDF function can be implemented in any programming language as long as it adheres to the Streaming-Runtime <code>gRPC</code> protocol.</p> <p>Following diagram visualizes the streaming-pipeline.yaml, implementing the iot-monitoring application with the help of <code>Stream</code> and <code>Processor</code> resources: </p>"},{"location":"samples/iot-monitoring/iot-monitoring/#quick-start","title":"Quick start","text":"<ul> <li> <p>Follow the Streaming Runtime Install instructions to instal the operator.</p> </li> <li> <p>Install the IoT monitoring streaming application:</p> </li> </ul> <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/streaming-pipeline.yaml' -n streaming-runtime\n</code></pre> <ul> <li> <p>Deploy a random data stream generator: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/iot-monitoring/data-generator.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. </p> </li> <li> <p>Delete all pipelines: <pre><code>kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app=iot-monitoring-data-generator -n streaming-runtime\n</code></pre></p> </li> </ul>"},{"location":"samples/iot-monitoring/iot-monitoring/#implementation-details","title":"Implementation details","text":"<p>The above scenario is implemented with the help of the <code>Streaming Runtime</code> using three <code>Streams</code> and two <code>Processor</code> resources. (Note: for the purpose of the demo we skip the explicit <code>CusterStream</code> definitions and instead will enable auth-provisioning for those).</p> <p>Given that the input <code>iot-monitoring-stream</code> uses an Avro data format like this:</p> <pre><code>namespace: com.tanzu.streaming.runtime.iot.log\ntype: record\nname: MonitoringStream\nfields:\n- name: error_code\ntype: string\n- name: ts\ntype:\ntype: long\nlogicalType: timestamp-millis\n- name: type\ntype: string\n- name: application\ntype: string\n- name: version\ntype: string\n- name: description\ntype: string\n</code></pre> <p>We can represent it with the following custom <code>Stream</code> resource with schema definition:</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: iot-monitoring-stream\nspec:\nprotocol: \"kafka\"\nstorage:\nclusterStream: \"iot-monitoring-cluster-stream\"\nstreamMode: [ \"read\" ]\nkeys: [ \"error_code\" ]\ndataSchemaContext:\nschema:\nnamespace: com.tanzu.streaming.runtime.iot.log\nname: MonitoringStream\nfields:\n- name: error_code\ntype: string\n- name: type\ntype: string\n- name: application\ntype: string\n- name: version\ntype: string\n- name: description\ntype: string\n- name: ts\ntype: long_timestamp-millis\nwatermark: \"`ts` - INTERVAL '3' SECONDS\"\noptions:\nddl.scan.startup.mode: earliest-offset\n</code></pre> <p>The <code>ts</code> field is the timestamp when the event was emitted. We are adding also a <code>3</code> seconds <code>watermark</code> to tolerate out-of-order or late coming events! </p> <p>The input events aggregation <code>Processor</code> can be defined like this: </p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: iot-monitoring-sql-aggregation\nspec:\ntype: FSQL\ninlineQuery:\n- \"INSERT INTO [[STREAM:error-count-stream]] SELECT window_start, window_end, error_code, COUNT(*) AS error_count FROM TABLE(TUMBLE(TABLE [[STREAM:iot-monitoring-stream]], DESCRIPTOR(ts), INTERVAL '1' MINUTE)) WHERE type='ERROR' GROUP BY window_start, window_end, error_code\"\n</code></pre> <p>It takes as input the <code>iot-monitoring-stream</code> and produces new <code>error-count-stream</code> stream populated with error counts aggregations, using <code>JSON</code> format:</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: error-count-stream\nspec:\nprotocol: \"kafka\"\nstorage:\nclusterStream: \"error-count-cluster-stream\"\nstreamMode: [ \"read\", \"write\" ]\nkeys: [ \"error_code\" ]\ndataSchemaContext:\nschema:\nnamespace: com.tanzu.streaming.runtime.iot.log\nname: ErrorCount\nfields:\n- name: window_start\ntype: long_timestamp-millis\n- name: window_end\ntype: long_timestamp-millis\n- name: error_code\ntype: string\n- name: error_count\ntype: long\noptions:\nddl.key.fields: error_code\nddl.value.format: \"json\"\nddl.properties.allow.auto.create.topics: \"true\"\nddl.scan.startup.mode: earliest-offset\n</code></pre> <p>Next the <code>iot-monitoring-udf</code> registers a custom <code>ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1</code> UDF that simply turns the payload to uppercase:</p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Processor\nmetadata:\nname: iot-monitoring-udf\nspec:\ntype: SRP\ninputs:\n- name: \"error-count-stream\"\noutputs:\n- name: \"udf-output-error-count-stream\"\ntemplate:\nspec:\ncontainers:\n- name: iot-monitoring-error-code-udf\nimage: ghcr.io/vmware-tanzu/streaming-runtimes/udf-uppercase-go:0.1\n</code></pre> <p>Note that the UDF function can be implemented in any programming language.</p> <p>Finally, the output of the UDF function is send to the <code>udf-output-error-count-stream</code> stream defined like this: </p> <pre><code>apiVersion: streaming.tanzu.vmware.com/v1alpha1\nkind: Stream\nmetadata:\nname: udf-output-error-count-stream\nspec:\nkeys: [ \"error_code\" ]\nstreamMode: [ \"write\" ]\nprotocol: \"rabbitmq\"\nstorage:\nclusterStream: \"udf-output-error-count-cluster-stream\"\n</code></pre> <p>It uses RabbitMQ message broker and doesn't define an explicit schema assuming the payload data is just a byte-array.</p>"},{"location":"samples/iot-monitoring/iot-monitoring/#next-step","title":"Next step","text":"<p>Explore the Music Chart use-case.</p>"},{"location":"samples/online-gaming-statistics/online-gaming-statistics/","title":"Online Game Plying Statistics","text":"<p>Processor types: SRP</p> <p>Doc: WIP</p> <p>The code snippets are fully functional but the documentation is still WIP.</p> <p>Find the code on GH.</p> <p>Collects, possibly out of order, data statistics from an online, multiplatform gaming platform and continuously computes temporal user and team scores statistics. As users play, we would like to maintain fresh, near real-time, view of current scores per user and per team.</p> <p>Lets assume the input <code>data-in-stream</code> stream has a format like this:</p> <pre><code>TODO\n...\n</code></pre> <p>We will leverage the SRP Processor's time-window aggregation capabilities to compute the scores per user and per team.  Furthermore we will take advantage of the data partitioning to scale the aggregation at hight throughput.</p> <p>Lets build a Data Pipeline that looks like this:</p> <p></p> <p>The <code>data-in-stream</code>'s filed <code>score_time</code> holds the time when the event was emitted.  Additionally a <code>watermark</code> (of <code>3</code> sec.) is configured to handle out-of-order or late coming events!</p>"},{"location":"samples/online-gaming-statistics/online-gaming-statistics/#quick-start","title":"Quick start","text":"<ul> <li> <p>Follow the Streaming Runtime Install instructions to instal the operator.</p> </li> <li> <p>Install the IoT monitoring streaming application: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/online-gaming-statistics/streaming-pipeline.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Deploy a random data stream generator: <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/online-gaming-statistics/data-generator.yaml' -n streaming-runtime\n</code></pre></p> </li> <li> <p>Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. </p> </li> <li> <p>Delete all pipelines: <pre><code>kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app=online-game-data-generator -n streaming-runtime\n</code></pre></p> </li> </ul>"},{"location":"samples/spring-cloud-stream/tick-tock/","title":"Ticktock Pipeline","text":"<p>The Spring Cloud Stream framework lets you easily build highly scalable event-driven microservices connected with shared messaging systems. Furthermore there is an extensive set (60+) of pre-built streaming applications that one can use out of the box. </p> <p>The Streaming-Runtime <code>Processor</code> resource provides seamless support for deploying Spring Cloud Stream application.</p> <p>Lest combine the time-source, transformer and log-sink streaming applications into a simple streaming pipeline:</p> <p></p> <p>The <code>time source</code> processor generates timestamps at fixed, pre-configured intervals and emits them to the <code>timestamps</code> stream .  The <code>time.date-format</code> property sets the desired date format.</p> <p>The <code>transformer</code> processor uses SpEL expressions to convert the input message payload and send the result to the output <code>uppercase</code> stream. Here the expression converts the payload to uppercase.</p> <p>The <code>log sink</code> processor listens for input messages from the <code>uppercase</code> stream and prints them to the standard console output.  The <code>log.expression</code> property allows us to specify the format of the printed log messages.</p> <p>The <code>Streams</code> exchange plain text payloads (e.g. byte-array) and do not require dedicated schema definitions.</p> <p>The pre-built SCS streaming applications are build to support same message broker as input and output stream.  For each application there are two builds one with <code>Apache Kafka</code> and another with <code>RabbitMQ</code>.  It is still possible to re-compile those applications for different message brokers (e.g. <code>protocols</code>) or even mixture for multiple protocols (e.g. <code>multibinder</code>).</p> <p>The streaming-pipeline-tiktock.yaml puts the entire pipeline together and after deployed would look like:</p> <p></p> <p>The streaming-pipeline-ticktock-partitioned-better.yaml shows how to data-partition the TickTock application leveraging the SCS Data-Partitioning capabilities.</p>"},{"location":"samples/spring-cloud-stream/tick-tock/#quick-start","title":"Quick start","text":"<ul> <li> <p>Follow the Streaming Runtime Install instructions to instal the operator.</p> </li> <li> <p>Install the anomaly detection streaming application:</p> ticktockticktock - partitioned <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/spring-cloud-stream/streaming-pipeline-ticktock.yaml' -n streaming-runtime\n</code></pre> <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/spring-cloud-stream/streaming-pipeline-ticktock-partitioned-better.yaml' -n streaming-runtime\n</code></pre> </li> <li> <p>Follow the explore results instructions to see what data is generated and how it is processed though the pipeline. </p> </li> <li> <p>To delete the data pipeline and the data generator:</p> </li> </ul> <pre><code>kubectl delete srs,srcs,srp --all -n streaming-runtime </code></pre>"},{"location":"samples/spring-cloud-stream/tick-tock/#next-step","title":"Next step","text":"<p>Explore the Online Game Statistics use-case.</p>"},{"location":"samples/top-k-songs/top-k-songs/","title":"Top-K Songs By Genre","text":"<p>Music ranking application that continuously computes the top 3 most played songs by genre, based on song play events collected in real-time.  (inspired by the music sample.)</p> <p>The application is modelled as a streaming music service with two input streams: <code>songs</code> and <code>playevents</code> and outputs <code>stream-out</code>.</p> <p>The <code>Stream</code> and <code>Processor</code> streaming runtime resources can help to model the music ranking applications. The desired data pipeline would look something like ths:</p> <p></p> <p>The <code>songs</code> stream is a feed of the songs known to the streaming service. It provides detailed information for each song. When a new song is released, the recording company sends a new song-event to the <code>songs</code> stream. The <code>playevents</code> stream on the other hand is a feed of songs being played by streaming service.</p> <p>The <code>song-join</code> Processor enriches the <code>playevents</code> stream by joining it with the <code>songs</code> stream.  The result <code>songplays</code> stream contains the streamed songs along with details such as song name and genre. The Processor uses streaming-SQL to implement the stream join operation:</p> <pre><code> INSERT INTO [[STREAM:songplays]] SELECT Plays.song_id, Songs.album, Songs.artist, Songs.name, Songs.genre, Plays.duration, Plays.event_time   FROM (\nSELECT * FROM [[STREAM:playevents]] WHERE duration &gt;= 30000\n) AS Plays INNER JOIN [[STREAM:songs]] AS Songs ON Plays.song_id = Songs.song_id\n</code></pre> <p>We, also, filter the play events to only accept events where the duration is &gt; 30 seconds.</p> <p>Next, the <code>song-aggregate</code> Processor groups the <code>songplays</code> stream by <code>name</code> and <code>genre</code> over a time-windowed interval and continuously compute the top 3 songs per genre over this interval. This effectively computes the top-k aggregate and when expressed in streaming SQL would look like this:</p> <pre><code> INSERT INTO [[STREAM:topk-songs-per-genre]] SELECT window_start, window_end, song_id, name, genre, play_count FROM ( SELECT *, ROW_NUMBER() OVER (PARTITION BY window_start, window_end, genre ORDER BY play_count DESC) AS row_num FROM ( SELECT window_start, window_end, song_id, name, genre, COUNT(*) AS play_count FROM TABLE(TUMBLE(TABLE [[STREAM:songplays]], DESCRIPTOR(event_time), INTERVAL '60' SECONDS)) GROUP BY window_start, window_end, song_id, name, genre ) ) WHERE row_num &lt;= 3\n</code></pre> <p>The aggregated <code>topk-songs-per-genre</code> stream emits every minute the top 3 songs per genre.</p> <p>Next the <code>song-udf</code> Processor is configured with a User Defined Function(UDF) to alter the payload programmatically send the result downstream to the <code>stream-out</code> stream.</p> <p>For this demo the <code>song-udf</code> Processor is configured with simple Python UDF that simply converts the input payload to uppercase:</p> <pre><code>class MessageService(MessageService_pb2_grpc.MessagingServiceServicer):\ndef requestReply(self, request, context):\nprint(\"Server received Payload: %s and Headers: %s\" % (request.payload.decode(), request.headers))\nreturn MessageService_pb2.GrpcMessage(\npayload=str.encode(request.payload.decode().upper()), headers=request.headers)\n</code></pre> <p>The UDF can be written in any programming language as long as it adhere to the User Defined Function contract.</p> <p>The streaming-pipeline.yaml uses the <code>Stream</code> and <code>Processor</code> resources to implement and deploy the music chart application:</p> <p></p>"},{"location":"samples/top-k-songs/top-k-songs/#quick-start","title":"Quick start","text":"<ul> <li> <p>Follow the Streaming Runtime Install instructions to instal the operator.</p> </li> <li> <p>Deploy the Music Chart streaming pipeline.</p> <p>Three alternative deployment configurations are provided to demonstrate different approaches to define the payload schemas.</p> streaming-pipeline.yamlwith SQL schemawith Avro schemawith Avro Schema Registry <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline.yaml' -n streaming-runtime\n</code></pre> <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline-inline-sql-schema.yaml' -n streaming-runtime\n</code></pre> <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline-inline-avro-schema.yaml' -n streaming-runtime\n</code></pre> <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/streaming-pipeline-inline-avro-confluent-schema.yaml' -n streaming-runtime\n</code></pre> <p>Note: you can choose between different Stream schema definitions approaches, selecting between sr-native, avro, sql-ddl and use remote schema registry.</p> </li> <li> <p>Start the input message generator. Messages are encoded in Avro, using the same schemas defined   by the <code>songs</code> and <code>playevents</code> Streams and send to the topics defined in those streams.</p> <pre><code>kubectl apply -f 'https://raw.githubusercontent.com/vmware-tanzu/streaming-runtimes/main/streaming-runtime-samples/top-k-songs/data-generator.yaml' -n streaming-runtime\n</code></pre> </li> <li> <p>Follow the instructions to explore the results.</p> <p>Use the <code>kubectl get srcs,srs,srp -n streaming-runtime</code> to list all Streaming Runtime resources:    <pre><code>kubectl get srcs,srs,srp -n streaming-runtime\nNAME                                                                                 READY   REASON\nclusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-playevents             true    ProtocolDeployed\nclusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-songplays              true    ProtocolDeployed\nclusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-songs                  true    ProtocolDeployed\nclusterstream.streaming.tanzu.vmware.com/cluster-stream-kafka-topk-songs-per-genre   true    ProtocolDeployed\nclusterstream.streaming.tanzu.vmware.com/cluster-stream-rabbitmq-out                 true    ProtocolDeployed\n\nNAME                                                                  READY   REASON\nstream.streaming.tanzu.vmware.com/kafka-stream-playevents             true    StreamDeployed\nstream.streaming.tanzu.vmware.com/kafka-stream-songplays              true    StreamDeployed\nstream.streaming.tanzu.vmware.com/kafka-stream-songs                  true    StreamDeployed\nstream.streaming.tanzu.vmware.com/kafka-stream-topk-songs-per-genre   true    StreamDeployed\nstream.streaming.tanzu.vmware.com/rabbitmq-stream-out                 true    StreamDeployed\n\nNAME                                                        READY   REASON\nprocessor.streaming.tanzu.vmware.com/topk-songs-aggregate   true    ProcessorDeployed\nprocessor.streaming.tanzu.vmware.com/topk-songs-join        true    ProcessorDeployed\nprocessor.streaming.tanzu.vmware.com/topk-songs-udf         true    ProcessorDeployed\n</code></pre></p> </li> <li> <p>Delete the music chart pipeline and the data generator:</p> <pre><code>kubectl delete srs,srcs,srp --all -n streaming-runtime kubectl delete deployments,svc -l app=top-k-songs-data-generator -n streaming-runtime\n</code></pre> </li> </ul>"},{"location":"samples/top-k-songs/top-k-songs/#next-step","title":"Next step","text":"<p>Explore the Spring CLoud Stream TickTock use-case.</p>"},{"location":"why/why-streaming-runtime/","title":"Why Streaming Runtime?","text":"<p>Microservices, containers, and Kubernetes help to free apps from infrastructure, enabling them to work independently and run anywhere. With Streaming Runtime, you can make the most of these cloud native patterns, automate the security and delivery of containerized streaming workloads, and proactively manage apps in production. It\u2019s all about freeing developers to do their thing: build great apps.</p>"},{"location":"why/why-streaming-runtime/#control-plane-vs-data-plane","title":"Control Plane vs Data Plane","text":"<p>As a design pattern, the <code>Control Plane</code> (CP) is a sub-system that defines and controls how the work should be done.  The <code>Data Plane</code> (DP) on the other hand is where the actual work is done.  The separation of concerns allows innovating and scaling both planes independently.</p> <p>In the context of the <code>Streaming Runtime</code> the <code>Data Plane</code> is where most of the data transformations happen and it is optimized for speed of processing, availability, simplicity and regularity.  This is where the event-driven applications and the messaging infrastructure work.   The SR <code>Control Plane</code> controls the <code>Data Plane</code> and is optimized for decision making and in general facilitating and simplifying the <code>Data Plane</code> processing. </p> <p>Kubernetes itself is designed around the Control Plane and Data Plane principles.  The CP is comprised of independent and composable process controllers that continuously drive the current state in the DP toward the provided desired state.  Controllers operate on a collection of API objects of a certain kind; for example, the built-in pods resource contains a collection of Pod objects.</p> <p>Kubernetes is also highly extensible, allowing to add new custom APIs and process controllers.  This provides us with a framework to build and run distributed applications resiliently!  The framework provides the building blocks for building developer platforms, but preserves user choice and flexibility where it is important.</p> <p>To take advantage of those capabilities the Streaming Runtime's CP is built as Kubernetes API extension (e.g. as Kubernetes Operator), with custom resources, such as Processor, Stream, ClusterStream CRDs, and reconciliation controllers for them (see the implementation stack).</p> <p>The SR Control Plane uses the custom resources as declarative policies to instantiate and tear down event-driven applications as needed, to provision the messaging middleware infrastructure and to manage the internal states of the pipelines in the Data Plane.</p> <p>Kubernetes takes care of scaling and failover and self-healing for Streaming Runtime CP&amp;DP, and provides deployment patterns, such as canary or blue/green deployments. </p> <p>Some of the benefits for building the SR's Control Plane as Kubernetes ApiServer extension include:</p> <ul> <li>common access control, audit logging, and policy extension</li> <li>access to the Kubernetes' own strongly-consistent storage via etcd, reducing the number of storage backends needed.</li> <li>common tools for managing API problems, such as validation and version changes.</li> <li>using the apiserver to host additional, custom, resources allows the resources to be managed with the same tooling as built-in resources.</li> <li> <p>existing rich ecosystem of Kubernetes operators that can be used to extend and compliment the SR capabilities. The SR already takes advantage of operators such as Service Binding, RabbitMQ Operator and Strimzi. </p> </li> <li> <p>kcp-dev/kcp - multi-tenant Kubernetes control plane for workloads on many clusters.</p> <p>With the power of CRDs, Kubernetes provides a flexible platform for declarative APIs of all types, and the reconciliation pattern common to Kubernetes controllers is a powerful tool in building robust, expressive systems. At the same time, a diverse and creative community of tools and services has sprung up around Kubernetes APIs.</p> </li> <li>Crossplane - let you build your own platform with your own opinionated concepts and APIs without needing to write a Kubernetes controller from scratch.</li> </ul>"},{"location":"why/why-streaming-runtime/#streaming-vs-batch-processing","title":"Streaming vs Batch Processing","text":"<p>In Batch processing the processing and analysis happens on a set of data that have already been stored over a period of time. An example is payroll and billing systems that have to be processed weekly or monthly. </p> <p>While the Table/Batch processing operates on data-at-rest ( e.g. bounded datasets), the streaming data processing operates on data-at-motion (unbounded datasets). </p> <p></p> <p>The stream processing is defined as the processing of an unbounded amount of data without interaction or interruption. </p> <p>Business cases for stream processing include: real-time credit card fraud detection or predictive analytics or near-real-time business data processing for actionable analytics.</p>"}]}