
logging.level.org.apache.flink=WARN
logging.level.org.apache.kafka=WARN
logging.level.org.apache.fink.metrics.MetricGroup=ERROR

sql.aggregation.kafkaServer=localhost:9094
sql.aggregation.schemaRegistry=http://localhost:8081
sql.aggregation.playEventsTopic=play-events
sql.aggregation.songsTopic=song-feed
sql.aggregation.outputSqlAggregateTopic=dataIn

sql.aggregation.executeSql[0]=CREATE TABLE Songs (\n \
   `the_kafka_key` STRING,\n \
   `id` BIGINT NOT NULL,\n \
   `album` STRING,\n \
   `artist` STRING,\n    \
   `name` STRING,\n    \
   `genre` STRING NOT NULL,\n   \
   `proctime` AS PROCTIME()\n    \
 ) WITH (\n   \
   'connector' = 'kafka',\n    \
   'topic' = '${sql.aggregation.songsTopic}',\n   \
   'properties.bootstrap.servers' = '${sql.aggregation.kafkaServer}',\n   \
   'properties.group.id' = 'testGroup',\n   \
   'key.format' = 'raw',\n   \
   'key.fields' = 'the_kafka_key',\n   \
   'value.format' = 'avro-confluent',\n   \
   'value.avro-confluent.url' = '${sql.aggregation.schemaRegistry}',\n   \
   'value.fields-include' = 'EXCEPT_KEY',\n   \
   'scan.startup.mode' = 'earliest-offset'\n    \
 )

# Input Songs (avro-confluent)
sql.aggregation.executeSql[1]=CREATE TABLE PlayEvents ( \n \
  `event_time` TIMESTAMP(3) METADATA FROM 'timestamp', \n \
  `the_kafka_key` STRING,  \n \
  `song_id` BIGINT NOT NULL,  \n \
  `duration` BIGINT,  \n \
  WATERMARK FOR `event_time` AS `event_time` - INTERVAL '30' SECONDS  \n \
) WITH ( \n \
  'connector' = 'kafka',  \n \
  'topic' = '${sql.aggregation.playEventsTopic}', \n \
  'properties.bootstrap.servers' = '${sql.aggregation.kafkaServer}', \n \
  'key.format' = 'raw', \n \
  'key.fields' = 'the_kafka_key', \n \
  'value.format' = 'avro-confluent', \n \
  'value.avro-confluent.url' = '${sql.aggregation.schemaRegistry}', \n \
  'value.fields-include' = 'EXCEPT_KEY', \n \
  'scan.startup.mode' = 'earliest-offset'  \n \
)

# Join table Sink
sql.aggregation.executeSql[2]=CREATE TABLE SongPlays ( \n \
  `song_id` BIGINT NOT NULL,  \n \
  `album` STRING,  \n \
  `artist` STRING,  \n \
  `name` STRING,  \n \
  `genre` STRING NOT NULL, \n \
  `duration` BIGINT,  \n \
  `event_time` TIMESTAMP(3) NOT NULL,  \n \
   WATERMARK FOR `event_time` AS `event_time` - INTERVAL '1' SECOND  \n \
) WITH ( \n \
   'connector' = 'kafka', \n \
   'topic' = 'play-events-genre-join', \n \
   'properties.bootstrap.servers' = '${sql.aggregation.kafkaServer}', \n \
   'properties.group.id' = 'testGroup3', \n \
   'properties.allow.auto.create.topics' = 'true', \n \
   'scan.startup.mode' = 'earliest-offset',  \n \
   'key.format' = 'json', \n \
   'key.fields' = 'song_id', \n \
   'key.json.ignore-parse-errors' = 'true', \n \
   'value.format' = 'json', \n \
   'value.json.fail-on-missing-field' = 'false', \n \
   'value.fields-include' = 'ALL' \n \
)

# SQL Aggregation Sink
sql.aggregation.executeSql[3]=CREATE TABLE TopKSongsPerGenre (  \n \
    `window_start` TIMESTAMP(3),   \n \
    `window_end` TIMESTAMP(3),   \n \
    `song_id` BIGINT NOT NULL,   \n \
    `name` STRING NOT NULL,  \n \
    `genre` STRING NOT NULL,  \n \
    `song_play_count` BIGINT,  \n \
    PRIMARY KEY (`window_start`, `window_end`, `song_id`, `genre`) NOT ENFORCED  \n \
 ) WITH (  \n \
    'connector' = 'upsert-kafka',  \n \
    'topic' = '${sql.aggregation.outputSqlAggregateTopic}',  \n \
    'properties.bootstrap.servers' = '${sql.aggregation.kafkaServer}',  \n \
    'properties.allow.auto.create.topics' = 'true', \n \
    'key.format' = 'json',  \n \
    'key.json.ignore-parse-errors' = 'true',  \n \
    'value.format' = 'json',  \n \
    'value.json.fail-on-missing-field' = 'false',  \n \
    'value.fields-include' = 'ALL'  \n \
 )

# Continuously Joins PlayEvents with Songs
sql.aggregation.executeSql[4]=INSERT INTO SongPlays \n \
    SELECT Plays.song_id, Songs.album, Songs.artist, Songs.name, Songs.genre, Plays.duration, Plays.event_time \n \
    FROM (SELECT * FROM PlayEvents WHERE duration >= 30000) AS Plays \n \
    INNER JOIN Songs ON Plays.song_id = Songs.id

# SQL Aggregation
sql.aggregation.executeSql[5]=INSERT INTO TopKSongsPerGenre   \n \
    SELECT window_start, window_end, song_id, name, genre, play_count   \n \
    FROM (  \n \
     SELECT *,   \n \
        ROW_NUMBER() OVER (PARTITION BY window_start, window_end, genre ORDER BY play_count DESC) AS row_num   \n \
     FROM (  \n \
        SELECT window_start, window_end, song_id, name, genre, COUNT(*) AS play_count   \n \
        FROM TABLE(   \n \
          TUMBLE(TABLE SongPlays, DESCRIPTOR(event_time), INTERVAL '60' SECONDS))   \n \
        GROUP BY window_start, window_end, song_id, name, genre   \n \
     )   \n \
    ) WHERE row_num <= 3

sql.aggregation.continuousQuery=SELECT * FROM TopKSongsPerGenre

sql.aggregation.explainStatements=4,5
